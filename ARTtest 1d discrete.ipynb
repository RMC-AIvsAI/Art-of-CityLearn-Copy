{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CityLearnART built using these commands\n",
    "- conda create -n CityLearnART python=3.10 setuptools==66 wheel<0.40 numpy pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.6 ipywidgets -c pytorch -c conda-forge\n",
    "- conda activate CityLearnART\n",
    "- pip install adversarial-robustness-toolbox[pytorch] citylearn==2.0b5 stable-baselines3[extra]==1.8.0\n",
    "\n",
    "restrictions (painful lessons):\n",
    "- CityLearn only works with PyTorch v1\n",
    "- CityLearn only works with SB3<2 ref: https://github.com/intelligent-environments-lab/CityLearn/issues/63, for this reason https://gymnasium.farama.org/content/migration-guide/#environment-reset\n",
    "- SB3 <2 requires gym 0.21 \n",
    "- gym requires setuptools==66 and wheel<0.40 ref:https://github.com/openai/gym/issues/3200, https://github.com/openai/gym/issues/3211, https://stackoverflow.com/questions/76129688/why-is-pip-install-gym-failing-with-python-setup-py-egg-info-did-not-run-succ \n",
    "- ART requires a version of numpy later than 1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C #Approaches SAC's performance in 2021 challenge benchmark, though PPO did well in this example: https://www.aicrowd.com/showcase/going-below-1-0-score-with-stablebaseline3\n",
    "\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.wrappers import NormalizedObservationWrapper, StableBaselines3Wrapper, DiscreteActionWrapper\n",
    "from citylearn.data import DataSet\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "from art.utils import to_categorical\n",
    "from art.attacks.evasion.brendel_bethge import BrendelBethgeAttack\n",
    "\n",
    "from KBMproject.mybb import BrendelBethgeAttack as mybba\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_name = 'baeda_3dem' #only in CityLearn v2\n",
    "dataset_name = 'citylearn_challenge_2022_phase_1' #only action is electrical storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_active_actions(schema: dict):\n",
    "    \"\"\"prints active actions in a CityLearn schema\"\"\"\n",
    "    print('Active actions:')\n",
    "    for key, value in schema['actions'].items():\n",
    "        if value['active']:\n",
    "            print('\\t',key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active actions:\n",
      "\t electrical_storage\n"
     ]
    }
   ],
   "source": [
    "schema = DataSet.get_schema(dataset_name)\n",
    "print_active_actions(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will ART's regressor work with a 1d action space? Ref for code below: https://colab.research.google.com/drive/1rZn6qLEIHMlu2iwNl1jKqvcEet8lS33A#scrollTo=A04LckaH0uuS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_active_actions(\n",
    "    schema: dict, active_actions: List[str]\n",
    ") -> dict:\n",
    "    \"\"\"Set the actions that will be part of the environment's \n",
    "    observation space that is provided to the control agent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    active_actions: List[str]\n",
    "        Names of actions to set active to be passed to control agent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with active actions set.\n",
    "    \"\"\"\n",
    "\n",
    "    active_count = 0\n",
    "\n",
    "    for o in schema['actions']:\n",
    "        if o in active_actions:\n",
    "            schema['actions'][o]['active'] = True\n",
    "            active_count += 1\n",
    "        else:\n",
    "            schema['actions'][o]['active'] = False\n",
    "\n",
    "    valid_actions = list(schema['actions'].keys())\n",
    "    assert active_count == len(active_actions),\\\n",
    "        'the provided actions are not all valid actions.'\\\n",
    "          f' Valid actionss in CityLearn are: {valid_actions}'\n",
    "    \n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This demonstrates how to change the active action, but this is unnecessary for citylearn_challenge_2022_phase_1\n",
    "#schema = set_active_actions(schema, ['electrical_storage']) \n",
    "#print_active_actions(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing active actions we ensure that there is only one device to control in each building, however multiple buildings will still result in the agent controlling multiple devices, so we mut remove all but one building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Building_1', 'Building_2', 'Building_3', 'Building_4', 'Building_5']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(schema['buildings'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_buildings(\n",
    "schema: dict, buildings_to_del: List[str]\n",
    ") -> dict:\n",
    "        \"\"\"removes buildings from a CityLeanr Schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    buildings_to_del: List[str]\n",
    "        Names of buildings to be removed from the environment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with buildings removed.\n",
    "    \"\"\"\n",
    "\n",
    "        print('Buildings in original schema: ', list(schema['buildings'].keys()))\n",
    "        schema['buildings'] = {key: value for key, value in schema['buildings'].items() if key not in buildings_to_del}\n",
    "        print('Buildings in new schema: ', list(schema['buildings'].keys()))\n",
    "\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all but building 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildings in original schema:  ['Building_1', 'Building_2', 'Building_3', 'Building_4', 'Building_5']\n",
      "Buildings in new schema:  ['Building_1']\n"
     ]
    }
   ],
   "source": [
    "schema = del_buildings(schema, list(schema['buildings'].keys())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set env parameters\n",
    "env = CityLearnEnv(schema, \n",
    "                   central_agent=True, #necessary for single agent\n",
    "                   #simulation_end_time_step=1000\n",
    "                   )\n",
    "#wrap env\n",
    "env = DiscreteActionWrapper(env, \n",
    "                            bin_sizes=[{'electrical_storage':20}] #this must be a list dicts for each action, see TabularQLearningWrapper example from https://colab.research.google.com/drive/1rZn6qLEIHMlu2iwNl1jKqvcEet8lS33A#scrollTo=1rkt9jnNuiZE\n",
    "                            )\n",
    "env = NormalizedObservationWrapper(env)\n",
    "env = StableBaselines3Wrapper(env)\n",
    "#Set RL algo parameters\n",
    "policy_kwargs = dict(net_arch=[256, 256])\n",
    "agent = A2C('MlpPolicy', \n",
    "            env,\n",
    "            device='cuda',\n",
    "            policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CityLearnEnv in module citylearn.citylearn:\n",
      "\n",
      "class CityLearnEnv(citylearn.base.Environment, gym.core.Env)\n",
      " |  CityLearnEnv(schema: Union[str, pathlib.Path, Mapping[str, Any]], root_directory: Union[str, pathlib.Path] = None, buildings: Union[List[citylearn.building.Building], List[str], List[int]] = None, simulation_start_time_step: int = None, simulation_end_time_step: int = None, episode_time_steps: Union[int, List[Tuple[int, int]]] = None, rolling_episode_split: bool = None, random_episode_split: bool = None, seconds_per_time_step: float = None, reward_function: citylearn.reward_function.RewardFunction = None, central_agent: bool = None, shared_observations: List[str] = None, random_seed: int = None, **kwargs: Any)\n",
      " |  \n",
      " |  CityLearn nvironment class.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  schema: Union[str, Path, Mapping[str, Any]]\n",
      " |      Name of CityLearn data set, filepath to JSON representation or :code:`dict` object of a CityLearn schema.\n",
      " |      Call :py:meth:`citylearn.data.DataSet.get_names` for list of available CityLearn data sets.\n",
      " |  root_directory: Union[str, Path]\n",
      " |      Absolute path to directory that contains the data files including the schema.\n",
      " |  buildings: Union[List[Building], List[str], List[int]], optional\n",
      " |      Buildings to include in environment. If list of :code:`citylearn.building.Building` is provided, will override :code:`buildings` definition in schema.\n",
      " |      If list of :str: is provided will include only schema :code:`buildings` keys that are contained in provided list of :code:`str`.\n",
      " |      If list of :int: is provided will include only schema :code:`buildings` whose index is contained in provided list of :code:`int`.\n",
      " |  simulation_start_time_step: int, optional\n",
      " |      Time step to start reading data files contents.\n",
      " |  simulation_end_time_step: int, optional\n",
      " |      Time step to end reading from data files contents.\n",
      " |  episode_time_steps: Union[int, List[Tuple[int, int]]], optional\n",
      " |      If type is `int`, it is the number of time steps in an episode. If type is `List[Tuple[int, int]]]` is provided, \n",
      " |      it is a list of episode start and end time steps between `simulation_start_time_step` and `simulation_end_time_step`. \n",
      " |      Defaults to (`simulation_end_time_step` - `simulation_start_time_step`) + 1. Will ignore `rolling_episode_split` if `episode_splits` is of type `List[Tuple[int, int]]]`.\n",
      " |  rolling_episode_split: bool, default: False\n",
      " |      True if episode sequences are split such that each time step is a candidate for `episode_start_time_step` otherwise, False to split episodes in steps of `episode_time_steps`.\n",
      " |  random_episode_split: bool, default: False\n",
      " |      True if episode splits are to be selected at random during training otherwise, False to select sequentially.\n",
      " |  seconds_per_time_step: float\n",
      " |      Number of seconds in 1 `time_step` and must be set to >= 1.\n",
      " |  reward_function: RewardFunction, optional\n",
      " |      Reward function class instance. If provided, will override :code:`reward_function` definition in schema.\n",
      " |  central_agent: bool, optional\n",
      " |      Expect 1 central agent to control all buildings.\n",
      " |  shared_observations: List[str], optional\n",
      " |      Names of common observations across all buildings i.e. observations that have the same value irrespective of the building.\n",
      " |  random_seed: int, optional\n",
      " |      Pseudorandom number generator seed for repeatable results.\n",
      " |  \n",
      " |  Other Parameters\n",
      " |  ----------------\n",
      " |  **kwargs : dict\n",
      " |      Other keyword arguments used to initialize super classes.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Parameters passed to `citylearn.citylearn.CityLearnEnv.__init__` that are also defined in `schema` will override their `schema` definition.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CityLearnEnv\n",
      " |      citylearn.base.Environment\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, schema: Union[str, pathlib.Path, Mapping[str, Any]], root_directory: Union[str, pathlib.Path] = None, buildings: Union[List[citylearn.building.Building], List[str], List[int]] = None, simulation_start_time_step: int = None, simulation_end_time_step: int = None, episode_time_steps: Union[int, List[Tuple[int, int]]] = None, rolling_episode_split: bool = None, random_episode_split: bool = None, seconds_per_time_step: float = None, reward_function: citylearn.reward_function.RewardFunction = None, central_agent: bool = None, shared_observations: List[str] = None, random_seed: int = None, **kwargs: Any)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evaluate(self, control_condition: citylearn.citylearn.EvaluationCondition = None, baseline_condition: citylearn.citylearn.EvaluationCondition = None, comfort_band: float = None) -> pandas.core.frame.DataFrame\n",
      " |      Evaluate cost functions at current time step.\n",
      " |      \n",
      " |      Calculates and returns building-level and district-level cost functions normalized w.r.t. the no control scenario.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      control_condition: EvaluationCondition, default: :code:`EvaluationCondition.WITH_STORAGE_AND_PARTIAL_LOAD_AND_PV`\n",
      " |          Condition for net electricity consumption, cost and emission to use in calculating cost functions for the control/flexible scenario.\n",
      " |      baseline_condition: EvaluationCondition, default: :code:`EvaluationCondition.WITHOUT_STORAGE_AND_PARTIAL_LOAD_BUT_WITH_PV`\n",
      " |          Condition for net electricity consumption, cost and emission to use in calculating cost functions for the baseline scenario \n",
      " |          that is used to normalize the control_condition scenario.\n",
      " |      comfort_band: float, default = 2.0\n",
      " |          Comfort band above and below dry_bulb_temperature_set_point beyond \n",
      " |          which occupant is assumed to be uncomfortable.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cost_functions: pd.DataFrame\n",
      " |          Cost function summary including the following: electricity consumption, zero net energy, carbon emissions, cost,\n",
      " |          discomfort (total, too cold, too hot, minimum delta, maximum delta, average delta), ramping, 1 - load factor,\n",
      " |          average daily peak and average annual peak.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The equation for the returned cost function values is :math:`\\frac{C_{\\textrm{control}}}{C_{\\textrm{no control}}}` \n",
      " |      where :math:`C_{\\textrm{control}}` is the value when the agent(s) control the environment and :math:`C_{\\textrm{no control}}`\n",
      " |      is the value when none of the storages and partial load cooling and heating devices in the environment are actively controlled.\n",
      " |  \n",
      " |  evaluate_citylearn_challenge(self) -> Mapping[str, Mapping[str, Union[str, float]]]\n",
      " |      Evalation function for The CityLearn Challenge 2023.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evaluation: Mapping[str, Mapping[str, Union[str, float]]]\n",
      " |          Mapping of internal CityLearn evaluation KPIs to their display name, weight and value.\n",
      " |  \n",
      " |  get_info(self) -> Mapping[Any, Any]\n",
      " |      Other information to return from the `citylearn.CityLearnEnv.step` function.\n",
      " |  \n",
      " |  get_metadata(self) -> Mapping[str, Any]\n",
      " |      Returns general static information.\n",
      " |  \n",
      " |  load_agent(self) -> 'citylearn.agents.base.Agent'\n",
      " |      Return :class:`Agent` or sub class object as defined by the `schema`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Parameters to override schema definitions. See :py:class:`citylearn.citylearn.CityLearnEnv` initialization parameters for valid kwargs.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      agents: Agent\n",
      " |          Simulation agent(s) for `citylearn_env.buildings` energy storage charging/discharging management.\n",
      " |  \n",
      " |  next_time_step(self)\n",
      " |      Advance all buildings to next `time_step`.\n",
      " |  \n",
      " |  render(self)\n",
      " |      Rendering function for The CityLearn Challenge 2023.\n",
      " |  \n",
      " |  reset(self) -> List[List[float]]\n",
      " |      Reset `CityLearnEnv` to initial state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      observations: List[List[float]]\n",
      " |          :attr:`observations`.\n",
      " |  \n",
      " |  step(self, actions: List[List[float]]) -> Tuple[List[List[float]], List[float], bool, dict]\n",
      " |      Apply actions to `buildings` and advance to next time step.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      actions: List[List[float]]\n",
      " |          Fractions of `buildings` storage devices' capacities to charge/discharge by. \n",
      " |          If `central_agent` is True, `actions` parameter should be a list of 1 list containing all buildings' actions and follows\n",
      " |          the ordering of buildings in `buildings`. If `central_agent` is False, `actions` parameter should be a list of sublists\n",
      " |          where each sublists contains the actions for each building in `buildings`  and follows the ordering of buildings in `buildings`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      observations: List[List[float]]\n",
      " |          :attr:`observations` current value.\n",
      " |      reward: List[float] \n",
      " |          :meth:`get_reward` current value.\n",
      " |      done: bool \n",
      " |          A boolean value for if the episode has ended, in which case further :meth:`step` calls will return undefined results.\n",
      " |          A done signal may be emitted for different reasons: Maybe the task underlying the environment was solved successfully,\n",
      " |          a certain timelimit was exceeded, or the physics simulation has entered an invalid observation.\n",
      " |      info: dict\n",
      " |          A dictionary that may contain additional information regarding the reason for a ``done`` signal.\n",
      " |          `info` contains auxiliary diagnostic information (helpful for debugging, learning, and logging).\n",
      " |          Override :meth\"`get_info` to get custom key-value pairs in `info`.\n",
      " |  \n",
      " |  update_variables(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_default_shared_observations() -> List[str]\n",
      " |      Names of default common observations across all buildings i.e. observations that have the same value irrespective of the building.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      May be used to assigned :attr:`shared_observations` value during `CityLearnEnv` object initialization.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  action_space\n",
      " |      Controller(s) action spaces.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      action_space : List[spaces.Box]\n",
      " |          List of agent(s) action spaces.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If `central_agent` is True, a list of 1 `spaces.Box` object is returned that contains all buildings' limits with the limits in the same order as `buildings`. \n",
      " |      If `central_agent` is False, a list of `space.Box` objects as many as `buildings` is returned in the same order as `buildings`.\n",
      " |  \n",
      " |  cooling_demand\n",
      " |      Summed `Building.cooling_demand`, in [kWh].\n",
      " |  \n",
      " |  cooling_electricity_consumption\n",
      " |      Summed `Building.cooling_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  cooling_storage_electricity_consumption\n",
      " |      Summed `Building.cooling_storage_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  dhw_demand\n",
      " |      Summed `Building.dhw_demand`, in [kWh].\n",
      " |  \n",
      " |  dhw_electricity_consumption\n",
      " |      Summed `Building.dhw_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  dhw_storage_electricity_consumption\n",
      " |      Summed `Building.dhw_storage_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  done\n",
      " |      Check if simulation has reached completion.\n",
      " |  \n",
      " |  electrical_storage_electricity_consumption\n",
      " |      Summed `Building.electrical_storage_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_cooling_device\n",
      " |      Summed `Building.energy_from_cooling_device` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_cooling_device_to_cooling_storage\n",
      " |      Summed `Building.energy_from_cooling_device_to_cooling_storage` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_cooling_storage\n",
      " |      Summed `Building.energy_from_cooling_storage` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_dhw_device\n",
      " |      Summed `Building.energy_from_dhw_device` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_dhw_device_to_dhw_storage\n",
      " |      Summed `Building.energy_from_dhw_device_to_dhw_storage` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_dhw_storage\n",
      " |      Summed `Building.energy_from_dhw_storage` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_electrical_storage\n",
      " |      Summed `Building.energy_from_electrical_storage` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_heating_device\n",
      " |      Summed `Building.energy_from_heating_device` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_heating_device_to_heating_storage\n",
      " |      Summed `Building.energy_from_heating_device_to_heating_storage` time series, in [kWh].\n",
      " |  \n",
      " |  energy_from_heating_storage\n",
      " |      Summed `Building.energy_from_heating_storage` time series, in [kWh].\n",
      " |  \n",
      " |  energy_to_electrical_storage\n",
      " |      Summed `Building.energy_to_electrical_storage` time series, in [kWh].\n",
      " |  \n",
      " |  episode\n",
      " |      Current episode index.\n",
      " |  \n",
      " |  heating_demand\n",
      " |      Summed `Building.heating_demand`, in [kWh].\n",
      " |  \n",
      " |  heating_electricity_consumption\n",
      " |      Summed `Building.heating_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  heating_storage_electricity_consumption\n",
      " |      Summed `Building.heating_storage_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  net_electricity_consumption\n",
      " |      Summed `Building.net_electricity_consumption` time series, in [kWh].\n",
      " |  \n",
      " |  net_electricity_consumption_cost\n",
      " |      Summed `Building.net_electricity_consumption_cost` time series, in [$].\n",
      " |  \n",
      " |  net_electricity_consumption_cost_without_storage\n",
      " |      Summed `Building.net_electricity_consumption_cost_without_storage` time series, in [$].\n",
      " |  \n",
      " |  net_electricity_consumption_cost_without_storage_and_partial_load\n",
      " |      Summed `Building.net_electricity_consumption_cost_without_storage_and_partial_load` time series, in [$].\n",
      " |  \n",
      " |  net_electricity_consumption_cost_without_storage_and_partial_load_and_pv\n",
      " |      Summed `Building.net_electricity_consumption_cost_without_storage_and_partial_load_and_pv` time series, in [$].\n",
      " |  \n",
      " |  net_electricity_consumption_cost_without_storage_and_pv\n",
      " |      Summed `Building.net_electricity_consumption_cost_without_storage_and_pv` time series, in [$].\n",
      " |  \n",
      " |  net_electricity_consumption_emission\n",
      " |      Summed `Building.net_electricity_consumption_emission` time series, in [kg_co2].\n",
      " |  \n",
      " |  net_electricity_consumption_emission_without_storage\n",
      " |      Summed `Building.net_electricity_consumption_emission_without_storage` time series, in [kg_co2].\n",
      " |  \n",
      " |  net_electricity_consumption_emission_without_storage_and_partial_load\n",
      " |      Summed `Building.net_electricity_consumption_emission_without_storage_and_partial_load` time series, in [kg_co2].\n",
      " |  \n",
      " |  net_electricity_consumption_emission_without_storage_and_partial_load_and_pv\n",
      " |      Summed `Building.net_electricity_consumption_emission_without_storage_and_partial_load_and_pv` time series, in [kg_co2].\n",
      " |  \n",
      " |  net_electricity_consumption_emission_without_storage_and_pv\n",
      " |      Summed `Building.net_electricity_consumption_emission_without_storage_and_pv` time series, in [kg_co2].\n",
      " |  \n",
      " |  net_electricity_consumption_without_storage\n",
      " |      Summed `Building.net_electricity_consumption_without_storage` time series, in [kWh].\n",
      " |  \n",
      " |  net_electricity_consumption_without_storage_and_partial_load\n",
      " |      Summed `Building.net_electricity_consumption_without_storage_and_partial_load` time series, in [kWh].\n",
      " |  \n",
      " |  net_electricity_consumption_without_storage_and_partial_load_and_pv\n",
      " |      Summed `Building.net_electricity_consumption_without_storage_and_partial_load_and_pv` time series, in [kWh].\n",
      " |  \n",
      " |  net_electricity_consumption_without_storage_and_pv\n",
      " |      Summed `Building.net_electricity_consumption_without_storage_and_pv` time series, in [kWh].\n",
      " |  \n",
      " |  non_shiftable_load_demand\n",
      " |      Summed `Building.non_shiftable_load_demand`, in [kWh].\n",
      " |  \n",
      " |  observation_names\n",
      " |      Names of returned observations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If `central_agent` is True, a list of 1 sublist containing all building observation names is returned in the same order as `buildings`. \n",
      " |      The `shared_observations` names are only included in the first building's observation names. If `central_agent` is False, a list of sublists \n",
      " |      is returned where each sublist is a list of 1 building's observation names and the sublist in the same order as `buildings`.\n",
      " |  \n",
      " |  observation_space\n",
      " |      Controller(s) observation spaces.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      observation_space : List[spaces.Box]\n",
      " |          List of agent(s) observation spaces.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If `central_agent` is True, a list of 1 `spaces.Box` object is returned that contains all buildings' limits with the limits in the same order as `buildings`. \n",
      " |      The `shared_observations` limits are only included in the first building's limits. If `central_agent` is False, a list of `space.Box` objects as\n",
      " |      many as `buildings` is returned in the same order as `buildings`.\n",
      " |  \n",
      " |  observations\n",
      " |      Observations at current time step.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If `central_agent` is True, a list of 1 sublist containing all building observation values is returned in the same order as `buildings`. \n",
      " |      The `shared_observations` values are only included in the first building's observation values. If `central_agent` is False, a list of sublists \n",
      " |      is returned where each sublist is a list of 1 building's observation values and the sublist in the same order as `buildings`.\n",
      " |  \n",
      " |  rewards\n",
      " |      Reward time series\n",
      " |  \n",
      " |  solar_generation\n",
      " |      Summed `Building.solar_generation, in [kWh]`.\n",
      " |  \n",
      " |  time_steps\n",
      " |      Number of time steps in current episode split.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  buildings\n",
      " |      Buildings in CityLearn environment.\n",
      " |  \n",
      " |  central_agent\n",
      " |      Expect 1 central agent to control all buildings.\n",
      " |  \n",
      " |  episode_time_steps\n",
      " |      If type is `int`, it is the number of time steps in an episode. If type is `List[Tuple[int, int]]]` is provided, it is a list of \n",
      " |      episode start and end time steps between `simulation_start_time_step` and `simulation_end_time_step`. Defaults to (`simulation_end_time_step` \n",
      " |      - `simulation_start_time_step`) + 1. Will ignore `rolling_episode_split` if `episode_splits` is of type `List[Tuple[int, int]]]`.\n",
      " |  \n",
      " |  episode_tracker\n",
      " |      :py:class:`citylearn.base.EpisodeTracker` object used to keep track of \n",
      " |      current episode time steps for reading observations from data files.\n",
      " |  \n",
      " |  random_episode_split\n",
      " |      True if episode splits are to be selected at random during training otherwise, False to select sequentially.\n",
      " |  \n",
      " |  reward_function\n",
      " |      Reward function class instance.\n",
      " |  \n",
      " |  rolling_episode_split\n",
      " |      True if episode sequences are split such that each time step is a candidate for `episode_start_time_step` otherwise, \n",
      " |      False to split episodes in steps of `episode_time_steps`.\n",
      " |  \n",
      " |  root_directory\n",
      " |      Absolute path to directory that contains the data files including the schema.\n",
      " |  \n",
      " |  schema\n",
      " |      Filepath to JSON representation or `dict` object of CityLearn schema.\n",
      " |  \n",
      " |  shared_observations\n",
      " |      Names of common observations across all buildings i.e. observations that have the same value irrespective of the building.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from citylearn.base.Environment:\n",
      " |  \n",
      " |  reset_time_step(self)\n",
      " |      Reset `time_step` to initial state.\n",
      " |      \n",
      " |      Sets `time_step` to 0.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from citylearn.base.Environment:\n",
      " |  \n",
      " |  time_step\n",
      " |      Current environment time step.\n",
      " |  \n",
      " |  uid\n",
      " |      Unique environment ID.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from citylearn.base.Environment:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  random_seed\n",
      " |      Pseudorandom number generator seed for repeatable results.\n",
      " |  \n",
      " |  seconds_per_time_step\n",
      " |      Number of seconds in 1 time step.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from gym.core.Env:\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  metadata = {'render.modes': []}\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CityLearnEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8760"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the default reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RewardFunction in module citylearn.reward_function object:\n",
      "\n",
      "class RewardFunction(builtins.object)\n",
      " |  RewardFunction(env_metadata: Mapping[str, Any], **kwargs)\n",
      " |  \n",
      " |  Base and default reward function class.\n",
      " |  \n",
      " |  The default reward is the electricity consumption from the grid at the current time step returned as a negative value.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  env_metadata: Mapping[str, Any]:\n",
      " |      General static information about the environment.\n",
      " |  **kwargs : dict\n",
      " |      Other keyword arguments for custom reward calculation.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, env_metadata: Mapping[str, Any], **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  calculate(self, observations: List[Mapping[str, Union[int, float]]]) -> List[float]\n",
      " |      Calculates reward.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      observations: List[Mapping[str, Union[int, float]]]\n",
      " |          List of all building observations at current :py:attr:`citylearn.citylearn.CityLearnEnv.\n",
      " |          time_step` that are got from calling :py:meth:`citylearn.building.Building.observations`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reward: List[float]\n",
      " |          Reward for transition to current timestep.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  central_agent\n",
      " |      Expect 1 central agent to control all buildings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  env_metadata\n",
      " |      General static information about the environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env.reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the action space is 1d for compatibility with ART:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert env.action_space.shape[0] == 1, f'action space is {env.action_space.shape[0]} dimensional, but should be 1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=256, out_features=20, bias=True)\n",
       "  (value_net): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (3): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.mlp_extractor.policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=20, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy.action_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=31, out_features=256, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=256, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "policy_net = deepcopy(agent.policy.mlp_extractor.policy_net) #copies shared net rather than referencing\n",
    "policy_net.add_module('4', agent.policy.action_net)\n",
    "policy_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization specifically for the PyTorch-based implementation:\n",
    "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/estimators/classification.html#pytorch-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "from art.estimators.classification import PyTorchClassifier as classifier\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "victim_policy = classifier(\n",
    "    model=policy_net,\n",
    "    loss=CrossEntropyLoss(), #most common func for classification\n",
    "    nb_classes=env.action_space[0].n,\n",
    "    input_shape=agent.observation_space.shape,\n",
    "    device_type='gpu',\n",
    "    clip_values = (agent.observation_space.low.min(),agent.observation_space.high.max())#(agent.observation_space.low,agent.observation_space.high) # arrays of  min and max values of each feature break the brendle bethge attack\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PyTorchClassifier in module art.estimators.classification.pytorch:\n",
      "\n",
      "class PyTorchClassifier(art.estimators.classification.classifier.ClassGradientsMixin, art.estimators.classification.classifier.ClassifierMixin, art.estimators.pytorch.PyTorchEstimator)\n",
      " |  PyTorchClassifier(model: 'torch.nn.Module', loss: 'torch.nn.modules.loss._Loss', input_shape: Tuple[int, ...], nb_classes: int, optimizer: Optional[ForwardRef('torch.optim.Optimizer')] = None, use_amp: bool = False, opt_level: str = 'O1', loss_scale: Union[float, str, NoneType] = 'dynamic', channels_first: bool = True, clip_values: Optional[ForwardRef('CLIP_VALUES_TYPE')] = None, preprocessing_defences: Union[ForwardRef('Preprocessor'), List[ForwardRef('Preprocessor')], NoneType] = None, postprocessing_defences: Union[ForwardRef('Postprocessor'), List[ForwardRef('Postprocessor')], NoneType] = None, preprocessing: 'PREPROCESSING_TYPE' = (0.0, 1.0), device_type: str = 'gpu') -> None\n",
      " |  \n",
      " |  This class implements a classifier with the PyTorch framework.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PyTorchClassifier\n",
      " |      art.estimators.classification.classifier.ClassGradientsMixin\n",
      " |      art.estimators.classification.classifier.ClassifierMixin\n",
      " |      art.estimators.pytorch.PyTorchEstimator\n",
      " |      art.estimators.estimator.NeuralNetworkMixin\n",
      " |      art.estimators.estimator.LossGradientsMixin\n",
      " |      art.estimators.estimator.BaseEstimator\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(self) -> Dict[str, Any]\n",
      " |      Use to ensure `PyTorchClassifier` can be pickled.\n",
      " |      \n",
      " |      :return: State dictionary with instance parameters.\n",
      " |  \n",
      " |  __init__(self, model: 'torch.nn.Module', loss: 'torch.nn.modules.loss._Loss', input_shape: Tuple[int, ...], nb_classes: int, optimizer: Optional[ForwardRef('torch.optim.Optimizer')] = None, use_amp: bool = False, opt_level: str = 'O1', loss_scale: Union[float, str, NoneType] = 'dynamic', channels_first: bool = True, clip_values: Optional[ForwardRef('CLIP_VALUES_TYPE')] = None, preprocessing_defences: Union[ForwardRef('Preprocessor'), List[ForwardRef('Preprocessor')], NoneType] = None, postprocessing_defences: Union[ForwardRef('Postprocessor'), List[ForwardRef('Postprocessor')], NoneType] = None, preprocessing: 'PREPROCESSING_TYPE' = (0.0, 1.0), device_type: str = 'gpu') -> None\n",
      " |      Initialization specifically for the PyTorch-based implementation.\n",
      " |      \n",
      " |      :param model: PyTorch model. The output of the model can be logits, probabilities or anything else. Logits\n",
      " |             output should be preferred where possible to ensure attack efficiency.\n",
      " |      :param loss: The loss function for which to compute gradients for training. The target label must be raw\n",
      " |             categorical, i.e. not converted to one-hot encoding.\n",
      " |      :param input_shape: The shape of one input instance.\n",
      " |      :param optimizer: The optimizer used to train the classifier.\n",
      " |      :param use_amp: Whether to use the automatic mixed precision tool to enable mixed precision training or\n",
      " |                      gradient computation, e.g. with loss gradient computation. When set to True, this option is\n",
      " |                      only triggered if there are GPUs available.\n",
      " |      :param opt_level: Specify a pure or mixed precision optimization level. Used when use_amp is True. Accepted\n",
      " |                        values are `O0`, `O1`, `O2`, and `O3`.\n",
      " |      :param loss_scale: Loss scaling. Used when use_amp is True. If passed as a string, must be a string\n",
      " |                         representing a number, e.g., “1.0”, or the string “dynamic”.\n",
      " |      :param nb_classes: The number of classes of the model.\n",
      " |      :param optimizer: The optimizer used to train the classifier.\n",
      " |      :param channels_first: Set channels first or last.\n",
      " |      :param clip_values: Tuple of the form `(min, max)` of floats or `np.ndarray` representing the minimum and\n",
      " |             maximum values allowed for features. If floats are provided, these will be used as the range of all\n",
      " |             features. If arrays are provided, each value will be considered the bound for a feature, thus\n",
      " |             the shape of clip values needs to match the total number of features.\n",
      " |      :param preprocessing_defences: Preprocessing defence(s) to be applied by the classifier.\n",
      " |      :param postprocessing_defences: Postprocessing defence(s) to be applied by the classifier.\n",
      " |      :param preprocessing: Tuple of the form `(subtrahend, divisor)` of floats or `np.ndarray` of values to be\n",
      " |             used for data preprocessing. The first value will be subtracted from the input. The input will then\n",
      " |             be divided by the second one.\n",
      " |      :param device_type: Type of device on which the classifier is run, either `gpu` or `cpu`.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state: Dict[str, Any]) -> None\n",
      " |      Use to ensure `PyTorchClassifier` can be unpickled.\n",
      " |      \n",
      " |      :param state: State dictionary with instance parameters to restore.\n",
      " |  \n",
      " |  class_gradient(self, x: numpy.ndarray, label: Union[int, List[int], NoneType] = None, training_mode: bool = False, **kwargs) -> numpy.ndarray\n",
      " |      Compute per-class derivatives w.r.t. `x`.\n",
      " |      \n",
      " |      :param x: Sample input with shape as expected by the model.\n",
      " |      :param label: Index of a specific per-class derivative. If an integer is provided, the gradient of that class\n",
      " |                    output is computed for all samples. If multiple values as provided, the first dimension should\n",
      " |                    match the batch size of `x`, and each value will be used as target for its corresponding sample in\n",
      " |                    `x`. If `None`, then gradients for all classes will be computed for each sample.\n",
      " |      :param training_mode: `True` for model set to training mode and `'False` for model set to evaluation mode.\n",
      " |                            Note on RNN-like models: Backpropagation through RNN modules in eval mode raises\n",
      " |                            RuntimeError due to cudnn issues and require training mode, i.e. RuntimeError: cudnn RNN\n",
      " |                            backward can only be called in training mode. Therefore, if the model is an RNN type we\n",
      " |                            always use training mode but freeze batch-norm and dropout layers if\n",
      " |                            `training_mode=False.`\n",
      " |      :return: Array of gradients of input features w.r.t. each class in the form\n",
      " |               `(batch_size, nb_classes, input_shape)` when computing for all classes, otherwise shape becomes\n",
      " |               `(batch_size, 1, input_shape)` when `label` parameter is specified.\n",
      " |  \n",
      " |  clone_for_refitting(self) -> 'PyTorchClassifier'\n",
      " |      Create a copy of the classifier that can be refit from scratch. Will inherit same architecture, same type of\n",
      " |      optimizer and initialization as the original classifier, but without weights.\n",
      " |      \n",
      " |      :return: new estimator\n",
      " |  \n",
      " |  compute_loss(self, x: Union[numpy.ndarray, ForwardRef('torch.Tensor')], y: Union[numpy.ndarray, ForwardRef('torch.Tensor')], reduction: str = 'none', **kwargs) -> Union[numpy.ndarray, ForwardRef('torch.Tensor')]\n",
      " |      Compute the loss.\n",
      " |      \n",
      " |      :param x: Sample input with shape as expected by the model.\n",
      " |      :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices\n",
      " |                of shape `(nb_samples,)`.\n",
      " |      :param reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n",
      " |                 'none': no reduction will be applied\n",
      " |                 'mean': the sum of the output will be divided by the number of elements in the output,\n",
      " |                 'sum': the output will be summed.\n",
      " |      :return: Array of losses of the same shape as `x`.\n",
      " |  \n",
      " |  compute_losses(self, x: Union[numpy.ndarray, ForwardRef('torch.Tensor')], y: Union[numpy.ndarray, ForwardRef('torch.Tensor')], reduction: str = 'none') -> Dict[str, Union[numpy.ndarray, ForwardRef('torch.Tensor')]]\n",
      " |      Compute all loss components.\n",
      " |      \n",
      " |      :param x: Sample input with shape as expected by the model.\n",
      " |      :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices\n",
      " |                of shape `(nb_samples,)`.\n",
      " |      :param reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n",
      " |                 'none': no reduction will be applied\n",
      " |                 'mean': the sum of the output will be divided by the number of elements in the output,\n",
      " |                 'sum': the output will be summed.\n",
      " |      :return: Dictionary of loss components.\n",
      " |  \n",
      " |  custom_loss_gradient(self, loss_fn, x: Union[numpy.ndarray, ForwardRef('torch.Tensor')], y: Union[numpy.ndarray, ForwardRef('torch.Tensor')], layer_name, training_mode: bool = False) -> Union[numpy.ndarray, ForwardRef('torch.Tensor')]\n",
      " |      Compute the gradient of the loss function w.r.t. `x`.\n",
      " |      \n",
      " |      :loss_fn: Loss function w.r.t to which gradient needs to be calculated.\n",
      " |      :param x: Sample input with shape as expected by the model(base image).\n",
      " |      :param y: Sample input with shape as expected by the model(target image).\n",
      " |      :param training_mode: `True` for model set to training mode and `'False` for model set to evaluation mode.`\n",
      " |      :param layer_name: Name of the layer from which activation needs to be extracted/activation layer.\n",
      " |      :return: Array of gradients of the same shape as `x`.\n",
      " |  \n",
      " |  fit = new_fit(self, *args, **kwargs)\n",
      " |      Fit the classifier on the training set `(x, y)`.\n",
      " |      \n",
      " |      :param x: Training data.\n",
      " |      :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or index labels of\n",
      " |                shape (nb_samples,).\n",
      " |      :param batch_size: Size of batches.\n",
      " |      :param nb_epochs: Number of epochs to use for training.\n",
      " |      :param training_mode: `True` for model set to training mode and `'False` for model set to evaluation mode.\n",
      " |      :param drop_last: Set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by\n",
      " |                        the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then\n",
      " |                        the last batch will be smaller. (default: ``False``)\n",
      " |      :param scheduler: Learning rate scheduler to run at the start of every epoch.\n",
      " |      :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\n",
      " |                     and providing it takes no effect.\n",
      " |  \n",
      " |  fit_generator(self, generator: 'DataGenerator', nb_epochs: int = 20, **kwargs) -> None\n",
      " |      Fit the classifier using the generator that yields batches as specified.\n",
      " |      \n",
      " |      :param generator: Batch generator providing `(x, y)` for each epoch.\n",
      " |      :param nb_epochs: Number of epochs to use for training.\n",
      " |      :param kwargs: Dictionary of framework-specific arguments. This parameter is not currently supported for PyTorch\n",
      " |             and providing it takes no effect.\n",
      " |  \n",
      " |  get_activations(self, x: Union[numpy.ndarray, ForwardRef('torch.Tensor')], layer: Union[int, str, NoneType] = None, batch_size: int = 128, framework: bool = False) -> Union[numpy.ndarray, ForwardRef('torch.Tensor')]\n",
      " |      Return the output of the specified layer for input `x`. `layer` is specified by layer index (between 0 and\n",
      " |      `nb_layers - 1`) or by name. The number of layers can be determined by counting the results returned by\n",
      " |      calling `layer_names`.\n",
      " |      \n",
      " |      :param x: Input for computing the activations.\n",
      " |      :param layer: Layer for computing the activations\n",
      " |      :param batch_size: Size of batches.\n",
      " |      :param framework: If true, return the intermediate tensor representation of the activation.\n",
      " |      :return: The output of `layer`, where the first dimension is the batch size corresponding to `x`.\n",
      " |  \n",
      " |  loss_gradient(self, x: Union[numpy.ndarray, ForwardRef('torch.Tensor')], y: Union[numpy.ndarray, ForwardRef('torch.Tensor')], training_mode: bool = False, **kwargs) -> Union[numpy.ndarray, ForwardRef('torch.Tensor')]\n",
      " |      Compute the gradient of the loss function w.r.t. `x`.\n",
      " |      \n",
      " |      :param x: Sample input with shape as expected by the model.\n",
      " |      :param y: Target values (class labels) one-hot-encoded of shape `(nb_samples, nb_classes)` or indices of shape\n",
      " |                `(nb_samples,)`.\n",
      " |      :param training_mode: `True` for model set to training mode and `'False` for model set to evaluation mode.\n",
      " |                            Note on RNN-like models: Backpropagation through RNN modules in eval mode raises\n",
      " |                            RuntimeError due to cudnn issues and require training mode, i.e. RuntimeError: cudnn RNN\n",
      " |                            backward can only be called in training mode. Therefore, if the model is an RNN type we\n",
      " |                            always use training mode but freeze batch-norm and dropout layers if\n",
      " |                            `training_mode=False.`\n",
      " |      :return: Array of gradients of the same shape as `x`.\n",
      " |  \n",
      " |  predict = new_predict(self, *args, **kwargs)\n",
      " |      Perform prediction for a batch of inputs.\n",
      " |      \n",
      " |      :param x: Input samples.\n",
      " |      :param batch_size: Size of batches.\n",
      " |      :param training_mode: `True` for model set to training mode and `'False` for model set to evaluation mode.\n",
      " |      :return: Array of predictions of shape `(nb_inputs, nb_classes)`.\n",
      " |  \n",
      " |  reduce_labels(self, y: Union[numpy.ndarray, ForwardRef('torch.Tensor')]) -> Union[numpy.ndarray, ForwardRef('torch.Tensor')]\n",
      " |      Reduce labels from one-hot encoded to index labels.\n",
      " |  \n",
      " |  reset(self) -> None\n",
      " |      Resets the weights of the classifier so that it can be refit from scratch.\n",
      " |  \n",
      " |  save(self, filename: str, path: Optional[str] = None) -> None\n",
      " |      Save a model to file in the format specific to the backend framework.\n",
      " |      \n",
      " |      :param filename: Name of the file where to store the model.\n",
      " |      :param path: Path of the folder where to store the model. If no path is specified, the model will be stored in\n",
      " |                   the default data location of the library `ART_DATA_PATH`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  device\n",
      " |      Get current used device.\n",
      " |      \n",
      " |      :return: Current used device.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Return the shape of one input sample.\n",
      " |      \n",
      " |      :return: Shape of one input sample.\n",
      " |  \n",
      " |  loss\n",
      " |      Return the loss function.\n",
      " |      \n",
      " |      :return: The loss function.\n",
      " |  \n",
      " |  loss_scale\n",
      " |      Return the loss scaling value.\n",
      " |      \n",
      " |      :return: Loss scaling. Possible values for string: a string representing a number, e.g., “1.0”,\n",
      " |               or the string “dynamic”.\n",
      " |  \n",
      " |  model\n",
      " |      Return the model.\n",
      " |      \n",
      " |      :return: The model.\n",
      " |  \n",
      " |  opt_level\n",
      " |      Return a string specifying a pure or mixed precision optimization level.\n",
      " |      \n",
      " |      :return: A string specifying a pure or mixed precision optimization level. Possible\n",
      " |               values are `O0`, `O1`, `O2`, and `O3`.\n",
      " |  \n",
      " |  optimizer\n",
      " |      Return the optimizer.\n",
      " |      \n",
      " |      :return: The optimizer.\n",
      " |  \n",
      " |  use_amp\n",
      " |      Return a boolean indicating whether to use the automatic mixed precision tool.\n",
      " |      \n",
      " |      :return: Whether to use the automatic mixed precision tool.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  estimator_params = ['model', 'clip_values', 'preprocessing_defences', ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from art.estimators.classification.classifier.ClassGradientsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from art.estimators.classification.classifier.ClassifierMixin:\n",
      " |  \n",
      " |  nb_classes\n",
      " |      Return the number of output classes.\n",
      " |      \n",
      " |      :return: Number of classes in the data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from art.estimators.pytorch.PyTorchEstimator:\n",
      " |  \n",
      " |  set_batchnorm(self, train: bool) -> None\n",
      " |      Set all batch normalization layers into train or eval mode.\n",
      " |      \n",
      " |      :param train: False for evaluation mode.\n",
      " |  \n",
      " |  set_dropout(self, train: bool) -> None\n",
      " |      Set all dropout layers into train or eval mode.\n",
      " |      \n",
      " |      :param train: False for evaluation mode.\n",
      " |  \n",
      " |  set_multihead_attention(self, train: bool) -> None\n",
      " |      Set all multi-head attention layers into train or eval mode.\n",
      " |      \n",
      " |      :param train: False for evaluation mode.\n",
      " |  \n",
      " |  set_params(self, **kwargs) -> None\n",
      " |      Take a dictionary of parameters and apply checks before setting them as attributes.\n",
      " |      \n",
      " |      :param kwargs: A dictionary of attributes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from art.estimators.pytorch.PyTorchEstimator:\n",
      " |  \n",
      " |  device_type\n",
      " |      Return the type of device on which the estimator is run.\n",
      " |      \n",
      " |      :return: Type of device on which the estimator is run, either `gpu` or `cpu`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from art.estimators.estimator.NeuralNetworkMixin:\n",
      " |  \n",
      " |  channels_first\n",
      " |      :return: Boolean to indicate index of the color channels in the sample `x`.\n",
      " |  \n",
      " |  layer_names\n",
      " |      Return the names of the hidden layers in the model, if applicable.\n",
      " |      \n",
      " |      :return: The names of the hidden layers in the model, input and output layers are ignored.\n",
      " |      \n",
      " |      .. warning:: `layer_names` tries to infer the internal structure of the model.\n",
      " |                   This feature comes with no guarantees on the correctness of the result.\n",
      " |                   The intended order of the layers tries to match their order in the model, but this is not\n",
      " |                   guaranteed either.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from art.estimators.estimator.BaseEstimator:\n",
      " |  \n",
      " |  compute_loss_from_predictions(self, pred: numpy.ndarray, y: numpy.ndarray, **kwargs) -> numpy.ndarray\n",
      " |      Compute the loss of the estimator for predictions `pred`.\n",
      " |      \n",
      " |      :param pred: Model predictions.\n",
      " |      :param y: Target values.\n",
      " |      :return: Loss values.\n",
      " |  \n",
      " |  get_params(self) -> Dict[str, Any]\n",
      " |      Get all parameters and their values of this estimator.\n",
      " |      \n",
      " |      :return: A dictionary of string parameter names to their value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from art.estimators.estimator.BaseEstimator:\n",
      " |  \n",
      " |  clip_values\n",
      " |      Return the clip values of the input samples.\n",
      " |      \n",
      " |      :return: Clip values (min, max).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.evasion import AutoProjectedGradientDescent as APGD\n",
    "from art.attacks.evasion import AutoConjugateGradient as ACG\n",
    "\n",
    "APGDatk = APGD(estimator=victim_policy, verbose=False)\n",
    "ACGatk = ACG(estimator=victim_policy, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.observation_space.shape == obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate attack using initial observation to verify pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same issue as I had in sinergym, need a different branch to solve the issue ref: https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/2165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input must have shape (n_sample, n_features), as obs is a 1d array of features with shape (20,) expand dims adds the n_samples for a shape of (1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36698732, 0.        , 0.5535534 , 0.        , 1.        ,\n",
       "        0.19999999, 0.2413534 , 0.77744365, 0.3466165 , 0.2413534 ,\n",
       "        0.5222222 , 1.        , 0.34444445, 0.48888886, 0.        ,\n",
       "        0.        , 1.        , 0.3       , 0.3       , 0.        ,\n",
       "        0.55519414, 0.3       , 0.17462254, 0.        , 0.        ,\n",
       "        0.        , 0.6901262 , 0.        , 0.        , 0.33030304,\n",
       "        0.33030304]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APGDatk.generate(np.expand_dims(obs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_obs = ACGatk.generate(np.expand_dims(obs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30530447, 0.24474017, 0.63920146, 0.12466008, 1.        ,\n",
       "        0.78491527, 0.40792447, 0.77744365, 0.3466165 , 0.36696437,\n",
       "        0.7207528 , 1.        , 0.5241754 , 0.9510994 , 0.22348298,\n",
       "        0.32458213, 1.        , 0.11122806, 0.3       , 0.35101303,\n",
       "        0.9915317 , 0.14146215, 0.77462256, 0.43223327, 0.09068313,\n",
       "        0.08863807, 0.6901262 , 0.31768602, 0.        , 0.33030304,\n",
       "        0.02581663]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5], dtype=int64), None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.predict(np.squeeze(adv_obs), deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(victim_policy.predict(adv_obs, training_mode=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiDiscrete([20])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action_space[0].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACGtarg = ACG(estimator=victim_policy, targeted=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9], dtype=int64), None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from art.utils import to_categorical\n",
    "targ_obs = ACGtarg.generate(np.expand_dims(obs, axis=0), to_categorical([19], nb_classes=agent.action_space[0].n) )\n",
    "agent.predict(np.squeeze(targ_obs), deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9], dtype=int64), None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.predict(np.squeeze(obs), deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BrendelBethgeAttack in module art.attacks.evasion.brendel_bethge:\n",
      "\n",
      "class BrendelBethgeAttack(art.attacks.attack.EvasionAttack)\n",
      " |  BrendelBethgeAttack(estimator: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', norm: Union[int, float, str] = inf, targeted: bool = False, overshoot: float = 1.1, steps: int = 1000, lr: float = 0.001, lr_decay: float = 0.5, lr_num_decay: int = 20, momentum: float = 0.8, binary_search_steps: int = 10, init_size: int = 100, batch_size: int = 32)\n",
      " |  \n",
      " |  Base class for the Brendel & Bethge adversarial attack [#Bren19]_, a powerful gradient-based adversarial attack that\n",
      " |  follows the adversarial boundary (the boundary between the space of adversarial and non-adversarial images as\n",
      " |  defined by the adversarial criterion) to find the minimum distance to the clean image.\n",
      " |  \n",
      " |  This is implementation of the Brendel & Bethge attack follows the reference implementation at\n",
      " |  https://github.com/bethgelab/foolbox/blob/master/foolbox/attacks/brendel_bethge.py.\n",
      " |  \n",
      " |  Implementation differs from the attack used in the paper in two ways:\n",
      " |  \n",
      " |  * The initial binary search is always using the full 10 steps (for ease of implementation).\n",
      " |  * The adaptation of the trust region over the course of optimisation is less\n",
      " |    greedy but is more robust, reliable and simpler (decay every K steps)\n",
      " |  \n",
      " |  References:\n",
      " |      .. [#Bren19] Wieland Brendel, Jonas Rauber, Matthias Kümmerer, Ivan Ustyuzhaninov, Matthias Bethge,\n",
      " |          \"Accurate, reliable and fast robustness evaluation\",\n",
      " |          33rd Conference on Neural Information Processing Systems (2019)\n",
      " |          https://arxiv.org/abs/1907.01003\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BrendelBethgeAttack\n",
      " |      art.attacks.attack.EvasionAttack\n",
      " |      art.attacks.attack.Attack\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator: 'CLASSIFIER_LOSS_GRADIENTS_TYPE', norm: Union[int, float, str] = inf, targeted: bool = False, overshoot: float = 1.1, steps: int = 1000, lr: float = 0.001, lr_decay: float = 0.5, lr_num_decay: int = 20, momentum: float = 0.8, binary_search_steps: int = 10, init_size: int = 100, batch_size: int = 32)\n",
      " |      :param estimator: A trained ART classifier providing loss gradients.\n",
      " |      :param norm: The norm of the adversarial perturbation. Possible values: \"inf\", np.inf, 1 or 2.\n",
      " |      :param targeted: Flag determining if attack is targeted.\n",
      " |      :param overshoot: If 1 the attack tries to return exactly to the adversarial boundary in each iteration. For\n",
      " |                        higher values the attack tries to overshoot over the boundary to ensure that the perturbed\n",
      " |                        sample in each iteration is adversarial.\n",
      " |      :param steps: Maximum number of iterations to run. Might converge and stop before that.\n",
      " |      :param lr: Trust region radius, behaves similar to a learning rate. Smaller values decrease the step size in\n",
      " |                 each iteration and ensure that the attack follows the boundary more faithfully.\n",
      " |      :param lr_decay: The trust region lr is multiplied with lr_decay in regular intervals (see lr_num_decay).\n",
      " |      :param lr_num_decay: Number of learning rate decays in regular intervals of length steps / lr_num_decay.\n",
      " |      :param momentum: Averaging of the boundary estimation over multiple steps. A momentum of zero would always take\n",
      " |                       the current estimate while values closer to one average over a larger number of iterations.\n",
      " |      :param binary_search_steps: Number of binary search steps used to find the adversarial boundary between the\n",
      " |                                  starting point and the clean image.\n",
      " |      :param init_size: Maximum number of random search steps to find initial adversarial example.\n",
      " |      :param batch_size: Batch size for evaluating the model for predictions and gradients.\n",
      " |  \n",
      " |  generate(self, x: numpy.ndarray, y: Optional[numpy.ndarray] = None, **kwargs) -> numpy.ndarray\n",
      " |      Applies the Brendel & Bethge attack.\n",
      " |      \n",
      " |      :param x: The original clean inputs.\n",
      " |      :param y: The labels for inputs `x`.\n",
      " |      \n",
      " |      :Keyword Arguments:\n",
      " |          * *starting_points* (``np.ndarray``)\n",
      " |              Optional. Adversarial inputs to use as a starting points, in particular for targeted attacks.\n",
      " |  \n",
      " |  mid_points(self, x0: numpy.ndarray, x1: numpy.ndarray, epsilons: numpy.ndarray, bounds: Tuple[float, float]) -> numpy.ndarray\n",
      " |      returns a point between x0 and x1 where epsilon = 0 returns x0 and epsilon = 1 returns x1\n",
      " |  \n",
      " |  norms(self, x: numpy.ndarray) -> numpy.ndarray\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  attack_params = ['norm', 'targeted', 'init_attack', 'overshoot', 'step...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from art.attacks.attack.EvasionAttack:\n",
      " |  \n",
      " |  targeted\n",
      " |      Return Boolean if attack is targeted. Return None if not applicable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from art.attacks.attack.Attack:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Returns a string describing the attack class and attack_params\n",
      " |  \n",
      " |  set_params(self, **kwargs) -> None\n",
      " |      Take in a dictionary of parameters and apply attack-specific checks before saving them as attributes.\n",
      " |      \n",
      " |      :param kwargs: A dictionary of attack-specific parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from art.attacks.attack.Attack:\n",
      " |  \n",
      " |  is_estimator_valid(estimator, estimator_requirements) -> bool\n",
      " |      Checks if the given estimator satisfies the requirements for this attack.\n",
      " |      \n",
      " |      :param estimator: The estimator to check.\n",
      " |      :param estimator_requirements: Estimator requirements.\n",
      " |      :return: True if the estimator is valid for the attack.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from art.attacks.attack.Attack:\n",
      " |  \n",
      " |  estimator\n",
      " |      The estimator.\n",
      " |  \n",
      " |  estimator_requirements\n",
      " |      The estimator requirements.\n",
      " |  \n",
      " |  summary_writer\n",
      " |      The summary writer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from art.attacks.attack.Attack:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BrendelBethgeAttack)\n",
    "bba = BrendelBethgeAttack(estimator=victim_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.29496175, 0.71825564, 0.01114884, 1.        ,\n",
       "        0.6352978 , 0.6766512 , 0.5470734 , 0.7819143 , 0.40605563,\n",
       "        0.68692446, 0.6535911 , 0.5091467 , 0.6535911 , 0.0772858 ,\n",
       "        0.15987988, 1.        , 0.13529778, 0.13529778, 0.        ,\n",
       "        0.78132814, 0.13529778, 0.6099203 , 0.14448342, 0.        ,\n",
       "        0.13529778, 0.8548284 , 0.1656008 , 0.        , 0.1656008 ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bba.generate(np.expand_dims(obs, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "testmybba = mybba(estimator=victim_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.2575361e-01, 1.9123371e-01, 7.9478717e-01, 2.0521297e-01,\n",
       "        9.9999982e-01, 5.5870229e-01, 6.0011965e-01, 5.3620988e-01,\n",
       "        7.0538288e-01, 4.8258722e-01, 8.8098848e-01, 8.4765518e-01,\n",
       "        5.8567822e-01, 7.3012257e-01, 1.1170563e-07, 8.3348423e-02,\n",
       "        8.8911974e-01, 5.8766317e-02, 5.8766324e-02, 1.6369812e-01,\n",
       "        9.1396046e-01, 5.8766276e-02, 4.1585621e-01, 3.3854747e-01,\n",
       "        4.8285676e-07, 7.4196670e-08, 9.3136013e-01, 8.9069150e-02,\n",
       "        3.0064498e-07, 8.9069128e-02, 2.1336032e-07]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmybba.generate(np.expand_dims(obs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Using model predictions as correct labels for FGM.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.17 s ± 15.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybba.generate(np.expand_dims(obs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Using model predictions as correct labels for FGM.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for untargeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.18 s ± 9.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bba.generate(np.expand_dims(obs, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for successful targeted attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([17], dtype=int64), None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.predict(np.squeeze(obs), deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n",
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "kwargs = kwargs = dict(norm= np.inf,\n",
    "        targeted=True, #default false\n",
    "        overshoot= 1.1,\n",
    "        steps=1000,\n",
    "        lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        lr_num_decay=20,\n",
    "        momentum=0.8,\n",
    "        binary_search_steps=10,\n",
    "        init_size=1_000_000, #default 100, finds sample matching the target class through iterative random search\n",
    "        #batch_size=1,\n",
    "        )\n",
    "\n",
    "bbat = BrendelBethgeAttack(estimator=victim_policy, **kwargs)\n",
    "testmybbat = mybba(estimator=victim_policy, **kwargs)\n",
    "\n",
    "target = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.86 s ± 604 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.12 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "26.5 s ± 23 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.47 s ± 48.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.35 s ± 36.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.46 s ± 12.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.12 s ± 1.32 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(norm= np.inf,\n",
    "        targeted=True, #default false\n",
    "        overshoot= 1.1,\n",
    "        steps=1000,\n",
    "        lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        lr_num_decay=20,\n",
    "        momentum=0.8,\n",
    "        binary_search_steps=10,\n",
    "        init_size=100_000, #default 100, finds sample matching the target class through iterative random search\n",
    "        #batch_size=1,\n",
    "        )\n",
    "\n",
    "target = [12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 s ± 15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n",
      "INFO:art.attacks.evasion.brendel_bethge:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.09 s ± 3.28 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14], dtype=int64), None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.predict(np.squeeze(obs), deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n",
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "kwargs = kwargs = dict(norm= np.inf,\n",
    "        targeted=True, #default false\n",
    "        overshoot= 1.1,\n",
    "        steps=1000,\n",
    "        lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        lr_num_decay=20,\n",
    "        momentum=0.8,\n",
    "        binary_search_steps=10,\n",
    "        init_size=1_000_000, #default 100, finds sample matching the target class through iterative random search\n",
    "        batch_size=32,\n",
    "        )\n",
    "\n",
    "bbat = BrendelBethgeAttack(estimator=victim_policy, **kwargs)\n",
    "testmybbat = mybba(estimator=victim_policy, **kwargs)\n",
    "\n",
    "target = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8 s ± 14.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n",
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "kwargs = kwargs = dict(norm= np.inf,\n",
    "        targeted=True, #default false\n",
    "        overshoot= 1.1,\n",
    "        steps=1000,\n",
    "        lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        lr_num_decay=20,\n",
    "        momentum=0.8,\n",
    "        binary_search_steps=10,\n",
    "        init_size=1_000_000, #default 100, finds sample matching the target class through iterative random search\n",
    "        batch_size=64,\n",
    "        )\n",
    "\n",
    "bbat = BrendelBethgeAttack(estimator=victim_policy, **kwargs)\n",
    "testmybbat = mybba(estimator=victim_policy, **kwargs)\n",
    "\n",
    "target = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.72 s ± 10.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n",
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "kwargs = kwargs = dict(norm= np.inf,\n",
    "        targeted=True, #default false\n",
    "        overshoot= 1.1,\n",
    "        steps=1000,\n",
    "        lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        lr_num_decay=20,\n",
    "        momentum=0.8,\n",
    "        binary_search_steps=10,\n",
    "        init_size=1_000_000, #default 100, finds sample matching the target class through iterative random search\n",
    "        batch_size=128,\n",
    "        )\n",
    "\n",
    "bbat = BrendelBethgeAttack(estimator=victim_policy, **kwargs)\n",
    "testmybbat = mybba(estimator=victim_policy, **kwargs)\n",
    "\n",
    "target = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.68 s ± 32.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n",
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "kwargs = kwargs = dict(norm= np.inf,\n",
    "        targeted=True, #default false\n",
    "        overshoot= 1.1,\n",
    "        steps=1000,\n",
    "        lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        lr_num_decay=20,\n",
    "        momentum=0.8,\n",
    "        binary_search_steps=10,\n",
    "        init_size=1_000_000, #default 100, finds sample matching the target class through iterative random search\n",
    "        batch_size=1000,\n",
    "        )\n",
    "\n",
    "bbat = BrendelBethgeAttack(estimator=victim_policy, **kwargs)\n",
    "testmybbat = mybba(estimator=victim_policy, **kwargs)\n",
    "\n",
    "target = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.65 s ± 7.46 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n",
      "INFO:art.estimators.classification.pytorch:Inferred 5 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "kwargs = kwargs = dict(norm= np.inf,\n",
    "        targeted=True, #default false\n",
    "        overshoot= 1.1,\n",
    "        steps=1000,\n",
    "        lr=1e-3,\n",
    "        lr_decay=0.5,\n",
    "        lr_num_decay=20,\n",
    "        momentum=0.8,\n",
    "        binary_search_steps=10,\n",
    "        init_size=1_000_000, #default 100, finds sample matching the target class through iterative random search\n",
    "        batch_size=10_000,\n",
    "        )\n",
    "\n",
    "bbat = BrendelBethgeAttack(estimator=victim_policy, **kwargs)\n",
    "testmybbat = mybba(estimator=victim_policy, **kwargs)\n",
    "\n",
    "target = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n",
      "INFO:KBMproject.mybb:Found initial adversarial image for targeted attack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.65 s ± 25.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "testmybbat.generate(np.expand_dims(obs, axis=0), \n",
    "y=to_categorical(target, nb_classes=agent.action_space[0].n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above seem to indicate that increasing the batch size has a small effect on the computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearnART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
