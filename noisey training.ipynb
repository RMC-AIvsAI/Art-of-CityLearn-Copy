{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback #, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from KBMproject import ATLA\n",
    "import KBMproject.utilities as utils\n",
    "\n",
    "from citylearn.data import DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2022_phase_2'\n",
    "SAVE_DIR = 'Models/noisey/'\n",
    "LOG_DIR = 'logs/Phase3/noisey/'\n",
    "VERBOSITY = 0\n",
    "#EVAL_VERBOSITY = 1\n",
    "DEVICE = 'cuda'\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 1 (1-12-21)\n",
    "Using the mean diff had results much worse than a comparable perturbation size in ATLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINS = 20\n",
    "EVALS = 10\n",
    "PRE_TRAINING_EPISODES = 0\n",
    "NOISEY_EPISODES = 300\n",
    "PRE_TRAINED_AGENT = None\n",
    "\n",
    "TOTAL_EPISODES = NOISEY_EPISODES + PRE_TRAINING_EPISODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define SB3 environments, note the the eval and training environments must be difference objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    schema=DataSet.get_schema(DATASET_NAME),\n",
    "    action_bins=BINS,\n",
    "    T=None #this was supposed to make evaluations shorter, but does not work... never passed it in lol\n",
    ")\n",
    "agent_env = utils.make_discrete_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        action_bins=BINS,\n",
    "                        seed=0)\n",
    "\n",
    "agent_eval_env = utils.make_discrete_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        action_bins=BINS,\n",
    "                        seed=42)\n",
    "\n",
    "if kwargs['T'] is not None:\n",
    "    print('T should be None unless this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each episode is 8759 timesteps\n"
     ]
    }
   ],
   "source": [
    "T = agent_env.time_steps - 1\n",
    "print(f'Each episode is {T} timesteps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define agent (could load/save pretrained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new agent defined\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINED_AGENT is None:\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "    agent = PPO('MlpPolicy', \n",
    "                agent_env,\n",
    "                device=DEVICE,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                tensorboard_log=LOG_DIR,\n",
    "                verbose=VERBOSITY,\n",
    "                )\n",
    "    print('new agent defined')\n",
    "else:\n",
    "    agent = PPO.load(path=PRE_TRAINED_AGENT,\n",
    "                     env=agent_env,\n",
    "                     device=DEVICE,\n",
    "                     tensorboard_log=LOG_DIR,\n",
    "                     verbose=VERBOSITY,\n",
    "                     print_system_info=True,\n",
    "                     #force_reset=False, #default is true for continued training ref: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.load\n",
    "                     )\n",
    "    print('agent loaded from storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dtg = f'{now.month}-{now.day}-{now.hour}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name contains RL algorithm, episodes per alternation and total episodes, followed by a the date-time with hour precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = f'{agent.__class__.__name__} '\n",
    "if PRE_TRAINING_EPISODES is not None:\n",
    "    agent_name += f'{PRE_TRAINING_EPISODES}+'\n",
    "agent_name += f'{NOISEY_EPISODES} {dtg}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretraining specified\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINING_EPISODES > 0:\n",
    "    print(f'Pre-training for {PRE_TRAINING_EPISODES*T} timesteps ({PRE_TRAINING_EPISODES} episodes)')\n",
    "    agent.learn(total_timesteps=TOTAL_EPISODES*T,\n",
    "                callback=[EvalCallback(Monitor(agent_eval_env),\n",
    "                                       eval_freq=PRE_TRAINING_EPISODES//EVALS*T,\n",
    "                                       verbose=VERBOSITY),\n",
    "                          ATLA.HParamCallback(),\n",
    "                          ATLA.PauseOnStepCallback(PRE_TRAINING_EPISODES*T)], #stops training before ts budget expended\n",
    "                tb_log_name=agent_name,\n",
    "                reset_num_timesteps=False, #allows training to continue where it left off\n",
    "                progress_bar=True,\n",
    "                log_interval=1 #start logging after first epsiode, for debugging\n",
    "                )\n",
    "    print(f'Agent pretrained for {agent.num_timesteps} timesteps, or {agent.num_timesteps/T} episodes')\n",
    "else:\n",
    "    print('No pretraining specified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mean difference resulted in far worse training than using the same value with ATLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = np.array([0.12511418, 0.12511418, 0.18184461, 0.35953119, 0.10637713,\n",
    "                     0.10636668, 0.15978021, 0.15978171, 0.15978064, 0.15977914,\n",
    "                     0.36344801, 0.36345118, 0.36344991, 0.36344801, 0.3260062 ,\n",
    "                     0.3260048 , 0.32599576, 0.3260062 , 0.44802713, 0.44802114,\n",
    "                     0.44802228, 0.44802713, 0.16781362, 0.36620854, 0.00152669,\n",
    "                     0.31896562, 0.00326229, 0.52109586, 0.52109586, 0.52109586,\n",
    "                     0.52109586])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the noise for the agent's environment, here we're using gaussian noise with a spread equal to the mean difference between two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_kwargs = dict(\n",
    "    loc=0, #mean\n",
    "    scale=0.05, #std, spread of the distribution\n",
    "    size=agent_env.observation_space.shape, #optiional when other params are array-like\n",
    ")\n",
    "perturbation = ATLA.summed_perturbation(np.random.normal,\n",
    "                                        agent_env.observation_space,\n",
    "                                        perturb_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap agent's environments for ATLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_env = ATLA.VictimATLAWrapper(agent_env,\n",
    "                                   perturbation,\n",
    "                                   )\n",
    "agent_eval_env = ATLA.VictimATLAWrapper(agent_eval_env,\n",
    "                                        perturbation,\n",
    "                                        )\n",
    "agent_eval_env = Monitor(agent_eval_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(agent_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace pre-training environment with ATLA environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_env(agent_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ATLA evaluation callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    eval_freq=TOTAL_EPISODES//EVALS*T,\n",
    "    verbose=VERBOSITY\n",
    ")\n",
    "\n",
    "agent_eval_callback = EvalCallback(agent_eval_env,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x206ad88e080>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = dict(\n",
    "    reset_num_timesteps=False, #allows training to continue where it left off between .learn() calls\n",
    "    progress_bar=False, # progress bar really slows cell execution\n",
    "    log_interval=1 #start logging after first epsiode, useful for debugging\n",
    ")\n",
    "\n",
    "\n",
    "agent.learn(total_timesteps=TOTAL_EPISODES*T,\n",
    "            callback=[agent_eval_callback,\n",
    "                      ATLA.HParamCallback(),\n",
    "                      ],\n",
    "                tb_log_name=agent_name,\n",
    "                **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbrod\\AppData\\Roaming\\Python\\Python310\\site-packages\\stable_baselines3\\common\\save_util.py:278: UserWarning: Path 'Models\\noisey' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "if SAVE_DIR is not None:\n",
    "    agent.save(SAVE_DIR + agent_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearnART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
