{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from KBMproject import ATLA\n",
    "import KBMproject.utilities as utils\n",
    "\n",
    "from citylearn.data import DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2022_phase_2'\n",
    "SAVE_DIR = 'Models/ATLA/'\n",
    "LOG_DIR = 'logs/Phase3/ATLA/'\n",
    "VERBOSITY = 0\n",
    "#EVAL_VERBOSITY = 1\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINS = 20\n",
    "R_EXP = 3 #for norm distance reward\n",
    "PRE_TRAINING_EPISODES = 0\n",
    "EVALS = 20\n",
    "\n",
    "\n",
    "PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "MAX_EPISODES = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define SB3 environments, note the the eval and training environments must be difference objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    schema=DataSet.get_schema(DATASET_NAME),\n",
    "    action_bins=BINS,\n",
    "    T=None #this was supposed to make evaluations shorter, but does not work... never passed it in lol\n",
    ")\n",
    "\n",
    "adv_env = utils.make_discrete_env(seed=0,\n",
    "                        **kwargs)\n",
    "\n",
    "adv_eval_env = utils.make_discrete_env(seed=42,\n",
    "                        **kwargs)\n",
    "if kwargs['T'] is not None:\n",
    "    print('T should be None unless this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each episode is 8759 timesteps\n"
     ]
    }
   ],
   "source": [
    "T = adv_env.time_steps - 1\n",
    "print(f'Each episode is {T} timesteps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define agent (could load/save pretrained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22631-SP0 10.0.22631\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.25.1\n",
      "- Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.19045-SP0 10.0.19045\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.0\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.25.1\n",
      "- Gym: 0.21.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = PPO.load(path=PRE_TRAINED_AGENT,\n",
    "                     #env=agent_env,\n",
    "                     device=DEVICE,\n",
    "                     tensorboard_log=LOG_DIR,\n",
    "                     verbose=VERBOSITY,\n",
    "                     print_system_info=True,\n",
    "                     #force_reset=False, #default is true for continued training ref: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.load\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of timesteps and agent has trained is non-zero when loaded from storage, this must be added to the pause and total timesteps so training is not prematurely aborted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_n_ts = agent.num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dtg = f'{now.month}-{now.day}-{now.hour}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name contains RL algorithm, episodes per alternation and total episodes, followed by a the date-time with hour precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary's reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwd = ATLA.NormScaleReward(adv_env, \n",
    "                            np.inf,\n",
    "                            exp=R_EXP,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose features which will be perturbed. The mask below leaves the temporal features unperturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=np.arange(6,31) #only features 7-31 will be perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an adv action space in [-1,1] for ATLA.BScaledSumPrevProj, which scale a maximum perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbrod\\AppData\\Roaming\\Python\\Python310\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "normalized_a_space = gym.spaces.Box(low=-1*np.ones(mask.shape),\n",
    "                                    high=np.ones(mask.shape),\n",
    "                                    dtype='float32',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameterize the B function\n",
    "- The adversary adds a bounded perturbation to the current observation with B(s) as BScaledSum\n",
    "- The max mean difference represents the largest change between two samples for each feature minus the mean difference. This will be the maximum perturbation size for our adversary. Using the max difference represents the wors case scenario we expect to encounter based on our training data. Because this is derived from the difference between samples, we subtract the mean difference so on average the inter sample change will not exceed the max recorded value. This is our boundary for the adversary's perturbation.\n",
    "see bline obs analysis.ipynb in the PPO 500 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mean_diff = np.array([0.24977164, 0.24977164, 0.34341758, 0.69515118, 0.04606484,\n",
    "                        0.04608573, 0.26690566, 0.26690266, 0.2669048 , 0.26690781,\n",
    "                        0.62865948, 0.62865314, 0.62865568, 0.62865948, 0.52596206,\n",
    "                        0.52596487, 0.52598294, 0.52596206, 0.75557218, 0.75558416,\n",
    "                        0.75558188, 0.75557218, 0.28202381, 0.61189055, 0.00253725,\n",
    "                        0.47459565, 0.0052361 , 0.89720221, 0.89720221, 0.89720221,\n",
    "                        0.89720221])\n",
    "\n",
    "mean_diff = np.array([0.12511418, 0.12511418, 0.18184461, 0.35953119, 0.10637713,\n",
    "                     0.10636668, 0.15978021, 0.15978171, 0.15978064, 0.15977914,\n",
    "                     0.36344801, 0.36345118, 0.36344991, 0.36344801, 0.3260062 ,\n",
    "                     0.3260048 , 0.32599576, 0.3260062 , 0.44802713, 0.44802114,\n",
    "                     0.44802228, 0.44802713, 0.16781362, 0.36620854, 0.00152669,\n",
    "                     0.31896562, 0.00326229, 0.52109586, 0.52109586, 0.52109586,\n",
    "                     0.52109586])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_params = dict(\n",
    "    max_perturbation=np.ones(mask.shape)*mean_diff[mask]*2\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    #adv_reward=rwd, #use default negative agent reward\n",
    "    victim=agent,\n",
    "    B=ATLA.BScaledSum,\n",
    "    action_space=normalized_a_space, #[-1,1] for scaled B defined above\n",
    "    feature_mask=mask, \n",
    "    B_kwargs=B_params,\n",
    ")\n",
    "adv_eval_env = ATLA.AdversaryATLAWrapper(env=adv_eval_env, **kwargs)\n",
    "adv_eval_env = Monitor(adv_eval_env)\n",
    "\n",
    "adv_env = ATLA.AdversaryATLAWrapper(env=adv_env, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(adv_env,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=[256, 256])\n",
    "adversary = SAC('MlpPolicy', \n",
    "            Monitor(adv_env),\n",
    "            device=DEVICE,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=VERBOSITY,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_name = f'{adversary.__class__.__name__} adversary {adversary.env.get_attr(\"B\")[0].__class__.__name__} 2x mean diff {dtg}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the adversary's perturbation function for the victim environment. We use a function which applies the corresponding B(s) to the adversary's prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ATLA evaluation callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_stopping = StopTrainingOnNoModelImprovement(max_no_improvement_evals=5, \n",
    "                                                min_evals=10,\n",
    "                                                verbose=2, #tell me why it stopped\n",
    "                                                )\n",
    "\n",
    "adv_eval_callback = EvalCallback(adv_eval_env,\n",
    "                                 eval_freq=MAX_EPISODES//EVALS*T,\n",
    "                                 callback_after_eval=adv_stopping,\n",
    "                                 verbose=VERBOSITY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct ATLA. Note:\n",
    "- the agents are not reset between iterations, this prevents attributes like scaled exploration and learning rates from resetting.\n",
    "- A callback pauses training after a number of episodes has elapsed but before the max training budget is reached (does this work better than resetting?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938bc670b71845589bcf8bfeb660979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1f267eeaf20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary.learn(total_timesteps=MAX_EPISODES*T,\n",
    "                callback=[adv_eval_callback,\n",
    "                            ATLA.AdvDistanceTensorboardCallback(),\n",
    "                            ATLA.HParamCallback(),\n",
    "                            ],\n",
    "                tb_log_name=adv_name,\n",
    "                reset_num_timesteps=False, #allows training to continue where it left off between .learn() calls\n",
    "                progress_bar=True, # progress bar really slows cell execution\n",
    "                log_interval=1 #start logging after first epsiode, useful for debugging\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DIR is not None:\n",
    "    adversary.save(SAVE_DIR + adv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training episodes before early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary.num_timesteps//T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearnART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
