{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_NAME = r'20 bin PPO 500 results\\default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "DATASET_NAME = 'citylearn_challenge_2022_phase_2' #only action is electrical storage\n",
    "SAVE_DIR = r'20 bin PPO 500 results\\binary classifier uACG results' + '/'\n",
    "ATK_NAME = 'untargeted_binary_myPGD_03_mask_time_REscale_solar_and_consumption_eps_clipped_adv_obs'\n",
    "CONSUMPTION_SPREAD = 0.016\n",
    "CONSUMPTION_IDX = 26\n",
    "SOLAR_IDX = 24\n",
    "SOLAR_SPREAD = 0.004 #0.04 prev typo\n",
    "MIN_OBS = 0\n",
    "MAX_OBS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from citylearn.data import DataSet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import KBMproject.utilities as utils\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = DataSet.get_schema(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO.load(path=f\"{AGENT_NAME}\")\n",
    "policy_net = utils.extract_actor(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = utils.make_discrete_env(schema=schema,  \n",
    "                        action_bins=agent.action_space[0].n,\n",
    "                        seed=42)\n",
    "cols = env.observation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pgd_linf(model, X, y=None, loss_fn=None, epsilon:float=0.05, step:float=0.01, num_iter:int=100, \n",
    "                num_decay:int=0, decay_rate=1):\n",
    "    \"\"\" Construct FGSM adversarial examples on the examples X with random restarts\n",
    "    ref: https://adversarial-ml-tutorial.org/adversarial_examples/\n",
    "    made for X as a single sample\"\"\"\n",
    "\n",
    "    assert 0 < decay_rate <= 1, 'decay rate must be between 0 and 1'\n",
    "    assert loss_fn is not None, 'Loss function must be provided'\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if y is None:\n",
    "        y = model(X)\n",
    "        n_out = y.shape[0]\n",
    "        if n_out > 1: #multiple outputs, assumes X is 1d\n",
    "            _, y = torch.max(y, -1) #argmax, max returns (values, indeces)\n",
    "            y = F.one_hot(y,num_classes=n_out)\n",
    "\n",
    "    if num_decay > 0: \n",
    "        decay_iters = num_iter//num_decay\n",
    "    else: #no decay\n",
    "        decay_iters = num_iter\n",
    "\n",
    "    delta = torch.zeros_like(X, requires_grad=True)\n",
    "    for iter in range(num_iter):\n",
    "\n",
    "        loss = loss_fn(reduction='none')(model(X + delta), y)\n",
    "        loss.backward(torch.ones_like(loss))\n",
    "\n",
    "        # Perform the update on delta (via the data attribute to skip the gradient tracking)\n",
    "        delta.data = (delta + step*delta.grad.detach().sign()).clamp(-epsilon, epsilon)\n",
    "        delta.grad.zero_()\n",
    "        \n",
    "        if(iter%decay_iters == 0):\n",
    "            step *= decay_rate\n",
    "        \n",
    "    return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaximumBifuricationWrapper(nn.Module):\n",
    "    \"\"\"modified to work with 1d samples\n",
    "    maybe add an option dim=1 kwarg in init for compatibility?\"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super(MaximumBifuricationWrapper, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.base_model(x)\n",
    "        lower_half, higher_half = torch.split(logits, logits.size(-1) // 2, dim=-1)\n",
    "        \n",
    "        # get the max of the lower and higher halves\n",
    "        lower_max = torch.max(lower_half, dim=-1)[0]\n",
    "        higher_max = torch.max(higher_half, dim=-1)[0]\n",
    "        \n",
    "        # concatenate the max of the lower and higher halves into a single tensor\n",
    "        output = torch.cat((lower_max.unsqueeze(-1), higher_max.unsqueeze(-1)), dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLLoss(nn.Module):\n",
    "    \"\"\"Carlini and Wagner or Difference Logits loss FOR UNTARGETED ATTACKS\n",
    "    where the loss is difference between the target/clean\n",
    "    logit and any other\n",
    "    this version is wtitten for 1d samples, ART uses 2d\"\"\"\n",
    "    def __init__(self, reduction=None):\n",
    "        super(DLLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target_one_hot): #myPGD doesn't provide a 1 hot target...\n",
    "        target_logits = torch.sum(target_one_hot * logits)\n",
    "        max_non_target_logits = torch.max((1 - target_one_hot) * logits)\n",
    "        loss = max_non_target_logits - target_logits\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  #reduction is None\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust epsilon for different features, the slice from 0 to 6 are temporal features and setting $\\epsilon$ to 0 means that these features will not be perturbed (using torch.clamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_list = np.ones(agent.observation_space.shape[0])*0.03\n",
    "eps_list[:6] = 0.0 #masked\n",
    "#these idx are improperly normalized, so eps bust be adjusted accordingly\n",
    "eps_list[SOLAR_IDX] *= SOLAR_SPREAD\n",
    "eps_list[CONSUMPTION_IDX] *= CONSUMPTION_SPREAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    model=MaximumBifuricationWrapper(policy_net), \n",
    "    epsilon=torch.tensor(eps_list, device=agent.device, dtype=torch.float32),\n",
    "    step=0.01,\n",
    "    num_iter=100,\n",
    "    num_decay=4,\n",
    "    decay_rate=0.5,\n",
    "    loss_fn=DLLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 8758/8759 [23:54<00:00,  6.10it/s] ASR=0.447]\n",
      "100%|██████████| 8759/8759 [23:54<00:00,  6.10it/s, ASR=0.447]\n"
     ]
    }
   ],
   "source": [
    "time_steps = None\n",
    "\n",
    "obs_list = []\n",
    "adv_obs_list = []\n",
    "a_list = []\n",
    "adv_a_list = []\n",
    "asr = 0\n",
    "n_features = agent.observation_space.shape[0]\n",
    "\n",
    "observations = env.reset()\n",
    "if time_steps is None:\n",
    "    time_steps = env.time_steps - 1\n",
    "\n",
    "pbar = tqdm(total=time_steps)\n",
    "for step in tqdm(range(time_steps)):\n",
    "\n",
    "    obs_list.append(observations)\n",
    "    actions = agent.predict(observations, deterministic=True)\n",
    "    a_list.append(actions[0])\n",
    "\n",
    "    delta = my_pgd_linf(X=torch.from_numpy(observations).to(agent.device),\n",
    "#try toggling the one hot target y based on odd/even steps to imitat the optimal adversarial policy\n",
    "                                         **kwargs).cpu().detach().numpy()\n",
    "    #adv_obs = observations + delta\n",
    "    adv_obs = np.clip(observations + delta, MIN_OBS, MAX_OBS) #keep adv obs in obs space\n",
    "    adv_obs_list.append(adv_obs)\n",
    "    \n",
    "    a_adv, _ = agent.predict(adv_obs, deterministic=True)\n",
    "    if a_adv[0] != actions[0]:\n",
    "        asr += 1\n",
    "\n",
    "    adv_a_list.append(a_adv[0])\n",
    "    observations, _, _, _ = env.step(a_adv)\n",
    "\n",
    "    #update progress bar including MAE\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix({'ASR': asr/(step + 1)}, refresh=True)\n",
    "    if env.done:\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "asr/=time_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost_function\n",
       "annual_peak_average                      1.000000\n",
       "carbon_emissions_total                   0.911016\n",
       "cost_total                               0.817345\n",
       "daily_one_minus_load_factor_average      1.025456\n",
       "daily_peak_average                       0.939530\n",
       "electricity_consumption_total            0.924670\n",
       "monthly_one_minus_load_factor_average    0.982108\n",
       "ramping_average                          1.187636\n",
       "zero_net_energy                          1.101910\n",
       "Name: District, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kpi = utils.format_kpis(env)\n",
    "display(kpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KPIs.csv updated\n"
     ]
    }
   ],
   "source": [
    "kpi_savename = SAVE_DIR+'KPIs.csv'\n",
    "try:\n",
    "    df_kpis = pd.read_csv(kpi_savename,\n",
    "                          index_col=0)\n",
    "    df_kpis[ATK_NAME] = kpi.values\n",
    "    df_kpis.to_csv(kpi_savename)\n",
    "    print('KPIs.csv updated')\n",
    "except:\n",
    "    kpi.name = ATK_NAME\n",
    "    kpi.to_csv(kpi_savename)\n",
    "    print('KPIs.csv created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs = pd.DataFrame(obs_list)\n",
    "df_obs.columns = cols\n",
    "df_obs['a'] = np.array(a_list).flatten().tolist()\n",
    "df_obs.to_csv(SAVE_DIR+ATK_NAME+'_obs-a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs = pd.DataFrame(adv_obs_list)\n",
    "df_obs.columns = cols\n",
    "df_obs['a'] = np.array(adv_a_list).flatten().tolist()\n",
    "df_obs.to_csv(SAVE_DIR+ATK_NAME+'_adv_obs-a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO agent 100 alts over 1000+200 2-3-21 results/ASRs.csv updated\n"
     ]
    }
   ],
   "source": [
    "asr_savename = SAVE_DIR+'ASRs.csv'\n",
    "try:\n",
    "    df_asrs = pd.read_csv(asr_savename,\n",
    "                          index_col=0)\n",
    "    df_asrs[ATK_NAME] = asr\n",
    "    df_asrs.to_csv(asr_savename)\n",
    "    print(f'{asr_savename} updated')\n",
    "except:\n",
    "    asr = pd.Series([asr])\n",
    "    asr.name = ATK_NAME\n",
    "    asr.to_csv(asr_savename)\n",
    "    print(f'{asr_savename} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_to_save = {k: v for k, v in kwargs.items() if k != 'model'} #don't save NN as json\n",
    "kwargs_to_save['loss_fn'] = kwargs['loss_fn'].__name__ #replace function with a string\n",
    "if not isinstance(kwargs_to_save['epsilon'], float):\n",
    "    kwargs_to_save['epsilon'] = eps_list.tolist() #tensors aren't json compatible, use list\n",
    "with open(SAVE_DIR+f'{ATK_NAME} parameters.json', 'w') as f:\n",
    "    json.dump(kwargs_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearnART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
