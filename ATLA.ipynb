{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from KBMproject import ATLA\n",
    "import KBMproject.utilities as utils\n",
    "\n",
    "from citylearn.data import DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2022_phase_2'\n",
    "SAVE_DIR = 'Models/ATLA/'\n",
    "LOG_DIR = 'logs/Phase3/ATLA/'\n",
    "VERBOSITY = 0\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max mean difference represents the largest change between two samples for each feature minus the mean difference. This will be the maximum perturbation size for our adversary. Using the max difference represents the wors case scenario we expect to encounter based on our training data. Because this is derived from the difference between samples, we subtract the mean difference so on average the inter sample change will not exceed the max recorded value. This is our boundary for the adversary's perturbation.\n",
    "see bline obs analysis.ipynb in the PPO 500 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEAN_DIFF = np.array([0.24977164, 0.24977164, 0.34341758, 0.69515118, 0.04606484,\n",
    "                        0.04608573, 0.26690566, 0.26690266, 0.2669048 , 0.26690781,\n",
    "                        0.62865948, 0.62865314, 0.62865568, 0.62865948, 0.52596206,\n",
    "                        0.52596487, 0.52598294, 0.52596206, 0.75557218, 0.75558416,\n",
    "                        0.75558188, 0.75557218, 0.28202381, 0.61189055, 0.00253725,\n",
    "                        0.47459565, 0.0052361 , 0.89720221, 0.89720221, 0.89720221,\n",
    "                        0.89720221])\n",
    "\n",
    "MEAN_DIFF = np.array([0.12511418, 0.12511418, 0.18184461, 0.35953119, 0.10637713,\n",
    "                     0.10636668, 0.15978021, 0.15978171, 0.15978064, 0.15977914,\n",
    "                     0.36344801, 0.36345118, 0.36344991, 0.36344801, 0.3260062 ,\n",
    "                     0.3260048 , 0.32599576, 0.3260062 , 0.44802713, 0.44802114,\n",
    "                     0.44802228, 0.44802713, 0.16781362, 0.36620854, 0.00152669,\n",
    "                     0.31896562, 0.00326229, 0.52109586, 0.52109586, 0.52109586,\n",
    "                     0.52109586]) #typo made this the average between the max and mean differences, rather than mean difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUG\n",
    "\n",
    "Whenever ATLA was resumed witha trained agent, the adversary was not given additional ts before training, so never changed it's policy. This explains why the only successful training involved multiple iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trials below used:\n",
    "adversary with BScaledSum bounded by max_mean_diff with normalized action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 1 (1-9-21?):\n",
    "- ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 50\n",
    "- N_ALT = 10\n",
    "\n",
    "The agent's training reward appeared close to convering after 20 episodes in ATLA, as per prev work, but the reward was not flat so ALT_EPISODES could be increased. Reward was exponentially increasing after 50 episodes so PRE_TRAINING_EPISODES could be increased. ATLA rewards and evals converged after 8 alternations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 2 (1-10-15?)\n",
    "- N_ALT = 10\n",
    "- ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 300\n",
    "\n",
    "Agent was closer to convergin with longer pre-training. Final eval/training rewards did not exceed trial one. Agent did not appear to converge after 10 alternations, perhaps this must be increased with PRE_TRAINING_EPISODES. The same was true within alternations, perhaps ALT_EPISODES must also increase. KPIs for this agent were worse than trail 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 3 (1-11-14?)\n",
    "- ALT_EPISODES = 30\n",
    "- N_ALT = 15\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 300 results\\default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_0.zip'\n",
    "\n",
    "Agent KPIs were similar to no coltroller, which is minimally exploitable, but also is also useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since 20 was too few and 300 too much, could 100 pretraining episodes be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trials below used:\n",
    "adversary with BScaledSum bounded by mean_diff with normalized action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 4 (1-12-21)\n",
    "- N_ALT = 10\n",
    "- ALT_EPISODES = 20\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 300 results\\default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_0.zip'\n",
    "\n",
    "With the smaller adv action space this agent is the best performer yet, slightly outperforming trial 1. It seems that the capability of the adversary psuh the agent into taking minimal actions to prevent being maipulated, which is why more training led to worse performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 5 (1-13-14) used half the mean difference in the adv action space\n",
    "- N_ALT = 10\n",
    "- ALT_EPISODES = 20\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 300 results\\default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_0.zip'\n",
    " \n",
    " While it's KPIs were lower than the pre-ATLA model, under the ACG attack the adversarial regret was reduced and it's KPIs were higher in the presence of the adversary. It's unclear if more ATLA alternations would improve convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 6 (1-14-15) Uses the pre-ATLA PPO 500 as the pre-trained agent, instead of the previous 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean diff/2 does not seem powerful enough as an attack, it only reduces performance by a few hundred points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 7 (1-16-10)\n",
    "- N_ALT = 10\n",
    "- ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "trained with mean diff, the eval score were still incresing slowly when training ended. Notable the CityLearn+ATLA paper showed the agent converging during each alt of 20 episodes, perhaps longer and fewr alts will perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 8 (1-17-9)\n",
    "- N_ALT = 3*\n",
    "- AGENT_ALT_EPISODES = 100* #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Agent plateaued after third iteration, adding alternations may improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 9 (1-17-21)\n",
    "- N_ALT = 5*\n",
    "- AGENT_ALT_EPISODES = 100 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Evals and mean reward were flat for 5th alternation. Produced the highest rewards so far, comparable to 1-14-15 which had half the perturbation space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 10 (1-18-21)\n",
    "- N_ALT = 5\n",
    "- AGENT_ALT_EPISODES = 100 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "- PERTURBATION_SCALE = 2*\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Reward was in the -7000s at the end of training, meaning that the agen't performance was too far below the baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All above masked time features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 11 (1-25-*)\n",
    "Same as trial 9 without any features masked\n",
    "- N_ALT = 5\n",
    "- AGENT_ALT_EPISODES = 100 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "- MASK=np.arange(0,31)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 12 (1-30-15)\n",
    "Same hyper-params as the most successful trial, but the training order of the adversary and agent are reversed to match the ATLA paper implementation. The agent will start training agsint the random perturbations of an untrained adversary\n",
    "- N_ALT = 5\n",
    "- AGENT_ALT_EPISODES = 100 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Agent might need another alt to coverge, since one was \"lost\" training against the random adversary. It's interesting that the rewards IMPROVED with the untrained/random adversary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trial 13 (2-1-9)\n",
    "\n",
    "Increase number of alts so eval scores are flat/converges\n",
    "\n",
    "- N_ALT = 7*\n",
    "- AGENT_ALT_EPISODES = 100 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = '20 bin PPO 500 results/default_PPO_citylearn_challenge_2022_phase_2_Building_6_20_bins_500.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Results were worse than the previous trial, instead try loading the previous agent and adversary, to continue alternations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST\n",
    "##### Trial 14 (2-3-03)\n",
    "\n",
    "- Continuation of trial 12, by loading agent and adversary\n",
    "- N_ALT = 2\n",
    "- AGENT_ALT_EPISODES = 100 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = 'Models\\ATLA\\PPO agent 100 alts over 500+500 1-30-15.zip'\n",
    "- PRE_TRAINED_ADV = 'Models\\ATLA\\SAC adversary 20 alts over 100 1-30-15.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Power consumption is the same as 12, but other metrics  improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINS = 20\n",
    "R_EXP = 3 #for norm distance reward\n",
    "\n",
    "N_ALT = 2\n",
    "AGENT_ALT_EPISODES = 100 #PPO take longer to converge than SAC\n",
    "ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "PRE_TRAINING_EPISODES = 0\n",
    "PRE_TRAINED_AGENT = 'Models\\ATLA\\PPO agent 100 alts over 500+500 1-30-15.zip'\n",
    "PRE_TRAINED_ADV = 'Models\\ATLA\\SAC adversary 20 alts over 100 1-30-15.zip'\n",
    "MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "PERTURBATION_SCALE = 1\n",
    "PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "EVAL_PER_ALT = 1\n",
    "ADV_TOTAL_EPISODES = ADV_ALT_EPISODES*N_ALT\n",
    "AGENT_TOTAL_EPISODES = AGENT_ALT_EPISODES*N_ALT + PRE_TRAINING_EPISODES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define SB3 environments, note the the eval and training environments must be difference objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    schema=DataSet.get_schema(DATASET_NAME),\n",
    "    action_bins=BINS,\n",
    "    T=None #this was supposed to make evaluations shorter, but does not work... never passed it in lol\n",
    ")\n",
    "agent_env = utils.make_discrete_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        action_bins=BINS,\n",
    "                        seed=0)\n",
    "\n",
    "agent_eval_env = utils.make_discrete_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        action_bins=BINS,\n",
    "                        seed=42)\n",
    "\n",
    "adv_env = utils.make_discrete_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        action_bins=BINS,\n",
    "                        seed=0)\n",
    "\n",
    "adv_eval_env = utils.make_discrete_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        action_bins=BINS,\n",
    "                        seed=42)\n",
    "if kwargs['T'] is not None:\n",
    "    print('T should be None unless this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each episode is 8759 timesteps\n"
     ]
    }
   ],
   "source": [
    "T = agent_env.time_steps - 1\n",
    "print(f'Each episode is {T} timesteps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define agent (could load/save pretrained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22631-SP0 10.0.22631\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.25.1\n",
      "- Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.19045-SP0 10.0.19045\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.5\n",
      "- Gym: 0.21.0\n",
      "\n",
      "agent loaded from storage\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINED_AGENT is None:\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "    agent = PPO('MlpPolicy', \n",
    "                agent_env,\n",
    "                device=DEVICE,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                tensorboard_log=LOG_DIR,\n",
    "                verbose=VERBOSITY,\n",
    "                )\n",
    "    print('new agent defined')\n",
    "else:\n",
    "    agent = PPO.load(path=PRE_TRAINED_AGENT,\n",
    "                     env=agent_env,\n",
    "                     device=DEVICE,\n",
    "                     tensorboard_log=LOG_DIR,\n",
    "                     verbose=VERBOSITY,\n",
    "                     print_system_info=True,\n",
    "                     #force_reset=False, #default is true for continued training ref: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.load\n",
    "                     )\n",
    "    print('agent loaded from storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of timesteps and agent has trained is non-zero when loaded from storage, this must be added to the pause and total timesteps so training is not prematurely aborted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_n_ts = agent.num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dtg = f'{now.month}-{now.day}-{now.hour}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name contains RL algorithm, episodes per alternation and total episodes, followed by a the date-time with hour precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = f'{agent.__class__.__name__} agent {AGENT_ALT_EPISODES} alts over '\n",
    "if PRE_TRAINING_EPISODES > 0:\n",
    "    agent_name += f'{PRE_TRAINING_EPISODES}+'\n",
    "else:\n",
    "    agent_name += f'{agent_n_ts//T}+'\n",
    "agent_name += f'{AGENT_ALT_EPISODES*N_ALT} {dtg}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretraining specified\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINING_EPISODES > 0:\n",
    "    print(f'Pre-training for {PRE_TRAINING_EPISODES*T} timesteps ({PRE_TRAINING_EPISODES} episodes)')\n",
    "    agent.learn(total_timesteps=AGENT_TOTAL_EPISODES*T + agent_n_ts,\n",
    "                callback=[EvalCallback(Monitor(agent_eval_env),\n",
    "                                       eval_freq=PRE_TRAINING_EPISODES//EVAL_PER_ALT*T,\n",
    "                                       verbose=VERBOSITY),\n",
    "                          ATLA.HParamCallback(),\n",
    "                          ATLA.PauseOnStepCallback(PRE_TRAINING_EPISODES*T + agent_n_ts)], #stops training before ts budget expended\n",
    "                tb_log_name=agent_name,\n",
    "                reset_num_timesteps=False, #allows training to continue where it left off\n",
    "                progress_bar=True,\n",
    "                log_interval=1 #start logging after first epsiode, for debugging\n",
    "                )\n",
    "    print(f'Agent pretrained for {agent.num_timesteps - agent_n_ts} timesteps, or {(agent.num_timesteps - agent_n_ts)/T} episodes')\n",
    "else:\n",
    "    print('No pretraining specified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary's reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwd = ATLA.NormScaleReward(adv_env, \n",
    "                            np.inf,\n",
    "                            exp=R_EXP,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an adv action space in [-1,1] for ATLA.BScaledSumPrevProj, which scale a maximum perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbrod\\AppData\\Roaming\\Python\\Python310\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "normalized_a_space = gym.spaces.Box(low=-1*np.ones(MASK.shape),\n",
    "                                    high=np.ones(MASK.shape),\n",
    "                                    dtype='float32',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameterize the B function\n",
    "- The adversary adds a bounded perturbation to the current observation with B(s) as BScaledSum\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max perturbation reduced to mean diff devided by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_params = dict(\n",
    "    #clip_bound=np.ones(agent_env.observation_space.shape)*0.33,\n",
    "    max_perturbation=np.ones(MASK.shape)*PERTURBATION_SPACE[MASK]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    #adv_reward=rwd, #use default negative agent reward\n",
    "    victim=agent,\n",
    "    B=ATLA.BScaledSum,\n",
    "    action_space=normalized_a_space, #[-1,1] for scaled B defined above\n",
    "    feature_mask=MASK, \n",
    "    B_kwargs=B_params,\n",
    ")\n",
    "adv_eval_env = ATLA.AdversaryATLAWrapper(env=adv_eval_env, **kwargs)\n",
    "adv_eval_env = Monitor(adv_eval_env)\n",
    "\n",
    "adv_env = ATLA.AdversaryATLAWrapper(env=adv_env, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_env(adv_env,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22631-SP0 10.0.22631\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.25.1\n",
      "- Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.19045-SP0 10.0.19045\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.5\n",
      "- Gym: 0.21.0\n",
      "\n",
      "agent loaded from storage\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINED_ADV is None:\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "    adversary = SAC('MlpPolicy', \n",
    "            Monitor(adv_env),\n",
    "            device=DEVICE,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=VERBOSITY,\n",
    "            )\n",
    "    print('new agent defined')\n",
    "else:\n",
    "    adversary = SAC.load(path=PRE_TRAINED_ADV,\n",
    "                     env=adv_env,\n",
    "                     device=DEVICE,\n",
    "                     tensorboard_log=LOG_DIR,\n",
    "                     verbose=VERBOSITY,\n",
    "                     print_system_info=True,\n",
    "                     #force_reset=False, #default is true for continued training ref: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.load\n",
    "                     )\n",
    "    print('agent loaded from storage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_name = f'{adversary.__class__.__name__} adversary {ADV_ALT_EPISODES} alts over {ADV_TOTAL_EPISODES} {dtg}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the adversary's perturbation function for the victim environment. We use a function which applies the corresponding B(s) to the adversary's prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation = ATLA.sb3_perturbation(adversary,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap agent's environments for ATLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perturbation=adversary.predict\n",
    "agent_env = ATLA.VictimATLAWrapper(agent_env,\n",
    "                                   perturbation,)\n",
    "agent_eval_env = ATLA.VictimATLAWrapper(agent_eval_env,\n",
    "                                        perturbation,)\n",
    "agent_eval_env = Monitor(agent_eval_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(agent_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace pre-training environment with ATLA environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_env(agent_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ATLA evaluation callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    verbose=VERBOSITY\n",
    ")\n",
    "\n",
    "adv_eval_callback = EvalCallback(adv_eval_env, \n",
    "                                 eval_freq=ADV_ALT_EPISODES//EVAL_PER_ALT*T, \n",
    "                                 **kwargs)\n",
    "agent_eval_callback = EvalCallback(agent_eval_env,\n",
    "                                   eval_freq=AGENT_ALT_EPISODES//EVAL_PER_ALT*T,\n",
    "                                   **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct ATLA. Note:\n",
    "- the agents are not reset between iterations, this prevents attributes like scaled exploration and learning rates from resetting.\n",
    "- A callback pauses training after a number of episodes has elapsed but before the max training budget is reached (does this work better than resetting?). \n",
    "- Adversary was originally trained first, so that the agent would start training against a trained adversary. However, the agent is trained first in the ATLA paper, implying it first faces a randomly initialized adversary. Try reversing them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLA for 2\n",
      "Agent trained has for 9636072 ts (1100.1338052289075 episodes) up to iteration 0\n",
      "Adversary has trained for 875901 ts (100.00011416828406 episodes) up to iteration 0\n",
      "Agent trained has for 10511972 ts (1200.1338052289075 episodes) up to iteration 1\n",
      "Adversary has trained for 875902 ts (100.0002283365681 episodes) up to iteration 1\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(\n",
    "    reset_num_timesteps=False, #allows training to continue where it left off between .learn() calls\n",
    "    progress_bar=False, # progress bar really slows cell execution\n",
    "    log_interval=1 #start logging after first epsiode, useful for debugging\n",
    ")\n",
    "print(f'ATLA for {N_ALT}')\n",
    "for alt in range(N_ALT):\n",
    "    #first trial had the agent train first, so these were reversed\n",
    "    agent.learn(total_timesteps=AGENT_TOTAL_EPISODES*T + agent_n_ts,\n",
    "                callback=[agent_eval_callback,\n",
    "                          ATLA.HParamCallback(),\n",
    "                          ATLA.PauseOnStepCallback(T*(AGENT_ALT_EPISODES*(1 + alt) + PRE_TRAINING_EPISODES) + agent_n_ts)],\n",
    "                    tb_log_name=agent_name,\n",
    "                    **kwargs)\n",
    "    print(f'Agent trained has for {agent.num_timesteps} ts ({agent.num_timesteps/T} episodes) up to iteration {alt}')\n",
    "\n",
    "    adversary.learn(total_timesteps=ADV_TOTAL_EPISODES*T,\n",
    "                    callback=[adv_eval_callback,\n",
    "                              ATLA.AdvDistanceTensorboardCallback(),\n",
    "                              ATLA.HParamCallback(),\n",
    "                              ATLA.PauseOnStepCallback(ADV_ALT_EPISODES*(1 + alt)*T)], #pauses training before the max ts, updates each iter\n",
    "                    tb_log_name=adv_name,\n",
    "                    **kwargs)\n",
    "    print(f'Adversary has trained for {adversary.num_timesteps} ts ({adversary.num_timesteps/T} episodes) up to iteration {alt}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DIR is not None:\n",
    "    agent.save(SAVE_DIR + agent_name)\n",
    "    adversary.save(SAVE_DIR + adv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearnART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
