{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import  SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from KBMproject import ATLA\n",
    "import KBMproject.utilities as utils\n",
    "\n",
    "from citylearn.data import DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2022_phase_2'\n",
    "SAVE_DIR = 'Models/ATLAc/'\n",
    "LOG_DIR = 'logs/Phase3/ATLAc/'\n",
    "VERBOSITY = 0\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max mean difference represents the largest change between two samples for each feature minus the mean difference. This will be the maximum perturbation size for our adversary. Using the max difference represents the wors case scenario we expect to encounter based on our training data. Because this is derived from the difference between samples, we subtract the mean difference so on average the inter sample change will not exceed the max recorded value. This is our boundary for the adversary's perturbation.\n",
    "see bline obs analysis.ipynb in the PPO 500 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEAN_DIFF = np.array([0.24977164, 0.24977164, 0.34341758, 0.69515118, 0.04606484,\n",
    "                        0.04608573, 0.26690566, 0.26690266, 0.2669048 , 0.26690781,\n",
    "                        0.62865948, 0.62865314, 0.62865568, 0.62865948, 0.52596206,\n",
    "                        0.52596487, 0.52598294, 0.52596206, 0.75557218, 0.75558416,\n",
    "                        0.75558188, 0.75557218, 0.28202381, 0.61189055, 0.00253725,\n",
    "                        0.47459565, 0.0052361 , 0.89720221, 0.89720221, 0.89720221,\n",
    "                        0.89720221])\n",
    "\n",
    "MEAN_DIFF = np.array([0.12511418, 0.12511418, 0.18184461, 0.35953119, 0.10637713,\n",
    "                     0.10636668, 0.15978021, 0.15978171, 0.15978064, 0.15977914,\n",
    "                     0.36344801, 0.36345118, 0.36344991, 0.36344801, 0.3260062 ,\n",
    "                     0.3260048 , 0.32599576, 0.3260062 , 0.44802713, 0.44802114,\n",
    "                     0.44802228, 0.44802713, 0.16781362, 0.36620854, 0.00152669,\n",
    "                     0.31896562, 0.00326229, 0.52109586, 0.52109586, 0.52109586,\n",
    "                     0.52109586]) #typo made this the average between the max and mean differences, rather than mean difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUG\n",
    "\n",
    "Whenever ATLA was resumed witha trained agent, the adversary was not given additional ts before training, so never changed it's policy. This explains why the only successful training involved multiple iterations. It also explains why the learning curves are different from the literature. Perhaps muuuuuuuch longer alternations are required  for a similar effect, so the agent recovers performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-10-13\n",
    "- N_ALT = 10\n",
    "- AGENT_ALT_EPISODES = 20 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = 'Models\\Victim\\SAC_citylearn_challenge_2022_phase_2_Building_6_default_rwd_MARLISA_hyperparams_500.zip'\n",
    "- PRE_TRAINED_ADV = None\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "KPIs were above one with scores in the ~-7200, trying more alts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-11-21\n",
    "\n",
    "- AGENT_ALT_EPISODES = 20 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = 'Models\\ATLAc\\2-10-13 SAC agent 20 alts over 500+200.zip'\n",
    "- PRE_TRAINED_ADV = 'Models\\ATLAc\\2-10-13 SAC agent 20 alts over 500+200.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 1\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Trying an additional 10 alts -> KPIs convergered to ~1, meaning the perturbations were too large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-12-9\n",
    "\n",
    "- N_ALT = 10\n",
    "- AGENT_ALT_EPISODES = 20 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\Victim\\SAC_citylearn_challenge_2022_phase_2_Building_6_default_rwd_MARLISA_hyperparams_500.zip'\n",
    "- PRE_TRAINED_ADV = None\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Same score as previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-13-7 -> 2-14-9 after GPU driver crash\n",
    "\n",
    "Previous may not have convergered in 20 episodes during the rebound, so we give the agent more training episodes, but fewer alts.\n",
    "\n",
    "- N_ALT = 7\n",
    "- AGENT_ALT_EPISODES = 30 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\Victim\\SAC_citylearn_challenge_2022_phase_2_Building_6_default_rwd_MARLISA_hyperparams_500.zip'\n",
    "- PRE_TRAINED_ADV = None\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-15-4\n",
    "\n",
    "See if agent from 2-14-9 continues to improve after 2 more alts\n",
    "\n",
    "- N_ALT = 2\n",
    "- AGENT_ALT_EPISODES = 30 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-14-9 SAC agent 30 alts over 500+210.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-14-9 SAC adversary 20 alts over 140.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-15-8\n",
    "\n",
    "Last trial improved performance, so we'll try a few more alts\n",
    "\n",
    "- N_ALT = 4\n",
    "- AGENT_ALT_EPISODES = 30 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-15-4 SAC agent 30 alts over 710+60.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-15-4 SAC agent 30 alts over 710+60.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-15-15 (best, but KPIs near 1)\n",
    "\n",
    "Last trial improved performance, so we'll try a few more alts again\n",
    "\n",
    "- N_ALT = 4\n",
    "- AGENT_ALT_EPISODES = 30 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-15-8 SAC agent 30 alts over 770+120.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-15-8 SAC adversary 20 alts over 80.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Small gains, perhaps restarting longer agent training will help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-15-22\n",
    "\n",
    "- N_ALT = 10\n",
    "- AGENT_ALT_EPISODES = 50*\n",
    "- ADV_ALT_EPISODES = 20 \n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\Victim\\SAC_citylearn_challenge_2022_phase_2_Building_6_default_rwd_MARLISA_hyperparams_500.zip'*\n",
    "- PRE_TRAINED_ADV = None\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "At half way the agent seems to have converged to a nil-action policy, and the extra training episodes might be detrimental. Unlike the previous trial, there is no upward trend (or small non-linear improvements). Stopped after 5 iterations as performance became flat.  Decrease perturbation space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-16-18\n",
    "\n",
    "Increasing agent training did not lead to better performance, so the perturbation size has been reduce for this attempt.\n",
    "\n",
    "- N_ALT = 10\n",
    "- AGENT_ALT_EPISODES = 30 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\Victim\\SAC_citylearn_challenge_2022_phase_2_Building_6_default_rwd_MARLISA_hyperparams_500.zip'\n",
    "- PRE_TRAINED_ADV = None\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.25\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Eval score still rapidly drops to -7000, adversary may still have too large a budget. Strangly this resulted in worse performance than 2-14-9/2-15-8 (which had twice the perturbation size), so the issue may not be perturbation size? Those trials had score near 1, so that may be the result of too large perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-17-21\n",
    "\n",
    "See if we can get agent KPIs above 1 with smaller perturbations\n",
    "\n",
    "- N_ALT = 10\n",
    "- AGENT_ALT_EPISODES = 30 #PPO take longer to converge than SAC\n",
    "- ADV_ALT_EPISODES = 20 #SAC adaquately converges in this time\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\Victim\\SAC_citylearn_challenge_2022_phase_2_Building_6_default_rwd_MARLISA_hyperparams_500.zip'\n",
    "- PRE_TRAINED_ADV = None\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.125*\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Samller B(s) resulted in smaller adv regret, but agent failed to improve from the reward floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-19-10 (cont 2-15-15)\n",
    "\n",
    "With no more promising agents, we'll continue where our best agent at (2-15-15) left off\n",
    "\n",
    "- N_ALT = 4\n",
    "- AGENT_ALT_EPISODES = 30\n",
    "- ADV_ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-15-15 SAC agent 30 alts over 890+120.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-15-15 SAC adversary 20 alts over 80.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "\n",
    "Electricity consumption is now below 1, may improve with further training. Performance is better under attack than unperturbed -> agent migh be over fitting as adv is not changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-19-19 -> 2-19-22 (due to power bump)\n",
    "\n",
    "since reloading agents to extend training 2-14-9 was not updating the adv due to a bug, we'll retraing from there with the predicted behaviour. Previous agents seem to overfit with the single adv. \n",
    "\n",
    "- N_ALT = 5\n",
    "- AGENT_ALT_EPISODES = 30\n",
    "- ADV_ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-14-9 SAC agent 30 alts over 500+210.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-14-9 SAC adversary 20 alts over 140.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "- TRAIN_LOADED_ADV = True #bug prevented adversary from training when loaded from storage, this allows this aspect to be enabled for continuing previous trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-20-7\n",
    "\n",
    "Error peristed, so rerunning the previous trial with fix\n",
    "\n",
    "- N_ALT = 5\n",
    "- AGENT_ALT_EPISODES = 30\n",
    "- ADV_ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-14-9 SAC agent 30 alts over 500+210.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-14-9 SAC adversary 20 alts over 140.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "- TRAIN_LOADED_ADV = True #bug prevented adversary from training when loaded from storage, this allows this aspect to be enabled for continuing previous trials\n",
    "\n",
    "performance is reducing, we can try again with longer agent training (learning I POMDP as the agent is harder than the MDP for the adversary) or see if continuing with the same adv as per 2-19-10 yields a usable agent in a clean setting. Maybe train it against a new adversary? Note that the adversary trains after the agent, so it's facing an unseen adversary when training starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2-20-21\n",
    "continuing from 2-19-10 (trace began with 2-15-15) with the adversary updating and extended number of episodes for the agent to train.\n",
    "\n",
    "- N_ALT = 2\n",
    "- AGENT_ALT_EPISODES = 100*\n",
    "- ADV_ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-19-10 SAC agent 30 alts over 1010+120.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-19-10 SAC adversary 20 alts over 80.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "- TRAIN_LOADED_ADV = True #bug prevented adversary from training when loaded from storage, this allows this aspect to be enabled for continuing previous \n",
    "\n",
    "Performance dropped and never recovered after the adversary was updated, the Agent need more time to recover before presented with a new adversary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-21-10\n",
    "\n",
    "Since and updated adversary collapsed performance even after 100 epsiodes of training togther (in 2-20-21), more time is required for the agent to adapt to a new adversary. Start again from 2-14-9, but with more training time for the agent? I'm worried the following agent overfit given how much a new adversary tanks performance. We'll start from 2-14-9, and training begins with a new adversary. THe agent's trining time will be increased (POMDPs are harder than MDPs)\n",
    "\n",
    "- N_ALT = 2\n",
    "- AGENT_ALT_EPISODES = 100\n",
    "- ADV_ALT_EPISODES = 20\n",
    "- PRE_TRAINING_EPISODES = 0\n",
    "- PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-14-9 SAC agent 30 alts over 500+210.zip'\n",
    "- PRE_TRAINED_ADV = r'Models\\ATLAc\\2-14-9 SAC adversary 20 alts over 140.zip'\n",
    "- MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "- PERTURBATION_SCALE = 0.5\n",
    "- PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "- TRAIN_LOADED_ADV = True #bug prevented adversary from training when loaded from storage, this allows this aspect to be enabled for continuing previous trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_EXP = 3 #for norm distance reward\n",
    "\n",
    "N_ALT = 2\n",
    "AGENT_ALT_EPISODES = 100\n",
    "ADV_ALT_EPISODES = 20\n",
    "PRE_TRAINING_EPISODES = 0\n",
    "PRE_TRAINED_AGENT = r'Models\\ATLAc\\2-14-9 SAC agent 30 alts over 500+210.zip'\n",
    "PRE_TRAINED_ADV = r'Models\\ATLAc\\2-14-9 SAC adversary 20 alts over 140.zip'\n",
    "MASK=np.arange(6,31) #only features 7-31 will be perturbed, temporal features left alone\n",
    "PERTURBATION_SCALE = 0.5\n",
    "PERTURBATION_SPACE = MEAN_DIFF*PERTURBATION_SCALE\n",
    "TRAIN_LOADED_ADV = True #bug prevented adversary from training when loaded from storage, this allows this aspect to be enabled for continuing previous trials\n",
    "\n",
    "EVAL_PER_ALT = 1\n",
    "ADV_TOTAL_EPISODES = ADV_ALT_EPISODES*N_ALT\n",
    "AGENT_TOTAL_EPISODES = AGENT_ALT_EPISODES*N_ALT + PRE_TRAINING_EPISODES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define SB3 environments, note the the eval and training environments must be difference objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    schema=DataSet.get_schema(DATASET_NAME),\n",
    "    T=None #this was supposed to make evaluations shorter, but does not work... never passed it in lol\n",
    ")\n",
    "agent_env = utils.make_continuous_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        seed=0)\n",
    "\n",
    "agent_eval_env = utils.make_continuous_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        seed=42)\n",
    "\n",
    "adv_env = utils.make_continuous_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        seed=0)\n",
    "\n",
    "adv_eval_env = utils.make_continuous_env(schema=DataSet.get_schema(DATASET_NAME),  \n",
    "                        seed=42)\n",
    "if kwargs['T'] is not None:\n",
    "    print('T should be None unless this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each episode is 8759 timesteps\n"
     ]
    }
   ],
   "source": [
    "T = agent_env.time_steps - 1\n",
    "print(f'Each episode is {T} timesteps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define agent (could load/save pretrained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.19045-SP0 10.0.19045\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.5\n",
      "- Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.19045-SP0 10.0.19045\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.5\n",
      "- Gym: 0.21.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Broda-Milian\\anaconda3\\envs\\CityLearnART\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent loaded from storage\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINED_AGENT is None:\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "    agent = SAC('MlpPolicy', \n",
    "                agent_env,\n",
    "                device=DEVICE,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                tensorboard_log=LOG_DIR,\n",
    "                verbose=VERBOSITY,\n",
    "                )\n",
    "    print('new agent defined')\n",
    "else:\n",
    "    agent = SAC.load(path=PRE_TRAINED_AGENT,\n",
    "                     env=agent_env,\n",
    "                     device=DEVICE,\n",
    "                     tensorboard_log=LOG_DIR,\n",
    "                     verbose=VERBOSITY,\n",
    "                     print_system_info=True,\n",
    "                     #force_reset=False, #default is true for continued training ref: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.load\n",
    "                     )\n",
    "    print('agent loaded from storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of timesteps and agent has trained is non-zero when loaded from storage, this must be added to the pause and total timesteps so training is not prematurely aborted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_n_ts = agent.num_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dtg = f'{now.month}-{now.day}-{now.hour}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name contains RL algorithm, episodes per alternation and total episodes, followed by a the date-time with hour precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = f'{dtg} {agent.__class__.__name__} agent {AGENT_ALT_EPISODES} alts over '\n",
    "if PRE_TRAINING_EPISODES > 0:\n",
    "    agent_name += f'{PRE_TRAINING_EPISODES}+'\n",
    "else:\n",
    "    agent_name += f'{agent_n_ts//T}+'\n",
    "agent_name += f'{AGENT_ALT_EPISODES*N_ALT}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretraining specified\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINING_EPISODES > 0:\n",
    "    print(f'Pre-training for {PRE_TRAINING_EPISODES*T} timesteps ({PRE_TRAINING_EPISODES} episodes)')\n",
    "    agent.learn(total_timesteps=AGENT_TOTAL_EPISODES*T + agent_n_ts,\n",
    "                callback=[EvalCallback(Monitor(agent_eval_env),\n",
    "                                       eval_freq=PRE_TRAINING_EPISODES//EVAL_PER_ALT*T,\n",
    "                                       verbose=VERBOSITY),\n",
    "                          ATLA.HParamCallback(),\n",
    "                          ATLA.PauseOnStepCallback(PRE_TRAINING_EPISODES*T + agent_n_ts)], #stops training before ts budget expended\n",
    "                tb_log_name=agent_name,\n",
    "                reset_num_timesteps=False, #allows training to continue where it left off\n",
    "                progress_bar=True,\n",
    "                log_interval=1 #start logging after first epsiode, for debugging\n",
    "                )\n",
    "    print(f'Agent pretrained for {agent.num_timesteps - agent_n_ts} timesteps, or {(agent.num_timesteps - agent_n_ts)/T} episodes')\n",
    "else:\n",
    "    print('No pretraining specified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary's reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwd = ATLA.NormScaleReward(adv_env, \n",
    "                            np.inf,\n",
    "                            exp=R_EXP,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an adv action space in [-1,1] for ATLA.BScaledSumPrevProj, which scale a maximum perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_a_space = gym.spaces.Box(low=-1*np.ones(MASK.shape),\n",
    "                                    high=np.ones(MASK.shape),\n",
    "                                    dtype='float32',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameterize the B function\n",
    "- The adversary adds a bounded perturbation to the current observation with B(s) as BScaledSum\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max perturbation reduced to mean diff devided by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_params = dict(\n",
    "    #clip_bound=np.ones(agent_env.observation_space.shape)*0.33,\n",
    "    max_perturbation=np.ones(MASK.shape)*PERTURBATION_SPACE[MASK]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary's environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    #adv_reward=rwd, #use default negative agent reward\n",
    "    victim=agent,\n",
    "    B=ATLA.BScaledSum,\n",
    "    action_space=normalized_a_space, #[-1,1] for scaled B defined above\n",
    "    feature_mask=MASK, \n",
    "    B_kwargs=B_params,\n",
    ")\n",
    "adv_eval_env = ATLA.AdversaryATLAWrapper(env=adv_eval_env, **kwargs)\n",
    "adv_eval_env = Monitor(adv_eval_env)\n",
    "\n",
    "adv_env = ATLA.AdversaryATLAWrapper(env=adv_env, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_env(adv_env,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.19045-SP0 10.0.19045\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.5\n",
      "- Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.19045-SP0 10.0.19045\n",
      "- Python: 3.10.12\n",
      "- Stable-Baselines3: 1.8.0\n",
      "- PyTorch: 1.12.1\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.5\n",
      "- Gym: 0.21.0\n",
      "\n",
      "adversary loaded from storage\n"
     ]
    }
   ],
   "source": [
    "if PRE_TRAINED_ADV is None:\n",
    "    policy_kwargs = dict(net_arch=[256, 256])\n",
    "    adversary = SAC('MlpPolicy', \n",
    "            Monitor(adv_env),\n",
    "            device=DEVICE,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=VERBOSITY,\n",
    "            )\n",
    "    print('new adversary defined')\n",
    "else:\n",
    "    adversary = SAC.load(path=PRE_TRAINED_ADV,\n",
    "                     env=adv_env,\n",
    "                     device=DEVICE,\n",
    "                     tensorboard_log=LOG_DIR,\n",
    "                     verbose=VERBOSITY,\n",
    "                     print_system_info=True,\n",
    "                     #force_reset=False, #default is true for continued training ref: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO.load\n",
    "                     )\n",
    "    print('adversary loaded from storage')\n",
    "if TRAIN_LOADED_ADV:\n",
    "    adv_n_ts = adversary.num_timesteps\n",
    "else: #legacy behaviour for pre 2-19-10 trials\n",
    "    adv_n_ts = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_name = f'{dtg} {adversary.__class__.__name__} adversary {ADV_ALT_EPISODES} alts over {ADV_TOTAL_EPISODES}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the adversary's perturbation function for the victim environment. We use a function which applies the corresponding B(s) to the adversary's prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation = ATLA.sb3_perturbation(adversary,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap agent's environments for ATLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perturbation=adversary.predict\n",
    "agent_env = ATLA.VictimATLAWrapper(agent_env,\n",
    "                                   perturbation,)\n",
    "agent_eval_env = ATLA.VictimATLAWrapper(agent_eval_env,\n",
    "                                        perturbation,)\n",
    "agent_eval_env = Monitor(agent_eval_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Broda-Milian\\anaconda3\\envs\\CityLearnART\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:384: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "check_env(agent_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace pre-training environment with ATLA environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_env(agent_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define ATLA evaluation callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    verbose=1 #print scores\n",
    ")\n",
    "\n",
    "adv_eval_callback = EvalCallback(adv_eval_env, \n",
    "                                 eval_freq=ADV_ALT_EPISODES//EVAL_PER_ALT*T, \n",
    "                                 **kwargs)\n",
    "agent_eval_callback = EvalCallback(agent_eval_env,\n",
    "                                   eval_freq=AGENT_ALT_EPISODES//EVAL_PER_ALT*T,\n",
    "                                   **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct ATLA. Note:\n",
    "- the agents are not reset between iterations, this prevents attributes like scaled exploration and learning rates from resetting.\n",
    "- A callback pauses training after a number of episodes has elapsed but before the max training budget is reached (does this work better than resetting?). \n",
    "- Adversary was originally trained first, so that the agent would start training against a trained adversary. However, the agent is trained first in the ATLA paper, implying it first faces a randomly initialized adversary. Try reversing them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLA for 2\n",
      "Eval num_timesteps=7094790, episode_reward=-6913.42 +/- 0.00\n",
      "Episode length: 8759.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Agent trained has for 7094790 ts (810.0 episodes) up to iteration 0\n",
      "agent progress saved\n",
      "Eval num_timesteps=1401440, episode_reward=8195.99 +/- 0.00\n",
      "Episode length: 8759.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Adversary has trained for 1401440 ts (160.0 episodes) up to iteration 0\n",
      "adversary progress saved\n",
      "Eval num_timesteps=7970690, episode_reward=-7184.25 +/- 0.00\n",
      "Episode length: 8759.00 +/- 0.00\n",
      "Agent trained has for 7970690 ts (910.0 episodes) up to iteration 1\n",
      "agent progress saved\n",
      "Eval num_timesteps=1576620, episode_reward=8341.98 +/- 0.00\n",
      "Episode length: 8759.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Adversary has trained for 1576620 ts (180.0 episodes) up to iteration 1\n",
      "adversary progress saved\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(\n",
    "    reset_num_timesteps=False, #allows training to continue where it left off between .learn() calls\n",
    "    progress_bar=False, # progress bar really slows cell execution\n",
    "    log_interval=1 #start logging after first epsiode, useful for debugging\n",
    ")\n",
    "print(f'ATLA for {N_ALT}')\n",
    "for alt in range(N_ALT):\n",
    "    #first trial had the agent train first, so these were reversed\n",
    "    agent.learn(total_timesteps=AGENT_TOTAL_EPISODES*T + agent_n_ts,\n",
    "                callback=[agent_eval_callback,\n",
    "                          ATLA.HParamCallback(),\n",
    "                          ATLA.PauseOnStepCallback(T*(AGENT_ALT_EPISODES*(1 + alt) + PRE_TRAINING_EPISODES) + agent_n_ts)],\n",
    "                    tb_log_name=agent_name,\n",
    "                    **kwargs)\n",
    "    print(f'Agent trained has for {agent.num_timesteps} ts ({agent.num_timesteps/T} episodes) up to iteration {alt}')\n",
    "    \n",
    "    if SAVE_DIR is not None:\n",
    "        try:\n",
    "            agent.save(SAVE_DIR + agent_name)\n",
    "            print('agent progress saved')\n",
    "        except:\n",
    "            print(f'error saving to {SAVE_DIR}')\n",
    "    \n",
    "    adversary.learn(total_timesteps=ADV_TOTAL_EPISODES*T + adv_n_ts, #missing +adv_n_ts, so the adversary is not actually training when loaded...\n",
    "                    callback=[adv_eval_callback,\n",
    "                              ATLA.AdvDistanceTensorboardCallback(),\n",
    "                              ATLA.HParamCallback(),\n",
    "                              ATLA.PauseOnStepCallback(ADV_ALT_EPISODES*(1 + alt)*T + adv_n_ts)], #pauses training before the max ts, updates each iter\n",
    "                    tb_log_name=adv_name,\n",
    "                    **kwargs)\n",
    "    print(f'Adversary has trained for {adversary.num_timesteps} ts ({adversary.num_timesteps/T} episodes) up to iteration {alt}')\n",
    "    \n",
    "    if SAVE_DIR is not None:\n",
    "        try:\n",
    "            adversary.save(SAVE_DIR + adv_name)\n",
    "            print('adversary progress saved')\n",
    "\n",
    "        except:\n",
    "            print(f'error saving to {SAVE_DIR}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_DIR is not None:\n",
    "    agent.save(SAVE_DIR + agent_name)\n",
    "    adversary.save(SAVE_DIR + adv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CityLearnART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
